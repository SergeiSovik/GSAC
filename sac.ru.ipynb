{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Discrete Soft Actor Critic with Prioritized Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уведомление об авторском праве\n",
    "Автор оригинальной работы: Совик Сергей, 2020 Июнь <sergeisovik@yahoo.com>\n",
    "\n",
    "Предоставляется разрешение на использование, копирование, изменение и/или распространение данного программного обеспечения для любых целей с оплатой или без оплаты при условии, что вышеупомянутое уведомление об авторских правах и это уведомление о разрешении присутствуют во всех копиях.\n",
    "\n",
    "Программное обеспечение предоставляется `как есть`, и автор отказывается от всех гарантий в отношении этого программного обеспечения, включая все подразумеваемые гарантии коммерческой выгоды и пригодности. Ни при каких обстоятельствах автор не несет ответственности за какие-либо особые, прямые или косвенные убытки или любые убытки, возникшие в результате потери данных или прибыли при использовании, будь то в результате действия договора, халатности или других вредоносных действий, возникающих в результате или в связи с использованием или работой данного программного обеспечения.\n",
    "\n",
    "# Предисловие\n",
    "Алгоритм пришлось реализовать на старых функциях Tensorflow с отключеным Eager режимом, в силу того, что новые функции Tensorflow 2.2 приводили к большой утечке памяти и выжирали 32 гб моей памяти буквально за один час.\n",
    "\n",
    "Все названия переменным и классам даны исходя из опыта программирования больше 25 лет, возможно непривычном, но интуитивно понятном для читателей виде и не являются общепринятым стандартом.\n",
    "\n",
    "Данная статья написана с целью расширения круга пользователей алгоритмом `Soft Actor Critic`, т.к. на момент написания статьи он является самым лучшим, а все существующие статьи написаны непонятным для многих программистов математическим языком.\n",
    "\n",
    "Для тех, кто совершенно не понимает, что такое `Вычислительный граф` и не хочет вдаваться в детальные подробности. Это модель описывающая взаимосвязь между всеми вычислениями, в том числе определяющая их порядок выполнения. Каждая операция в котором называется `Узел`. `Вычислительный граф` чемто напоминает блок схему с множеством возможных входов и выходов. Таким образом, запрашивая вычислить результат `Узла` у нейросетевого движка, производится вычисление всех зависимостей, и при необходимости запрашиваются входные данные.\n",
    "\n",
    "# Алгоритм `Genetic Soft Actor Critic`\n",
    "Алгоритм реализован в виде одного цельного графа, что позволяет уменьшить объём обмена данных с GPU, и ускорить процесс обучения.\n",
    "\n",
    "Алгоритм состоит из четырёх основных блоков:\n",
    "- Блок `Нейросеть`\n",
    "- Блок `Игрок`\n",
    "- Блок `Генетический буфер повтора`\n",
    "- Блок `Тренер`\n",
    "\n",
    "Каждый из которых может работать параллельно-последовательно.\n",
    "\n",
    "## Блок `Нейросеть`\n",
    "Нейросеть состоит из нескольких независимо обучаемых блоков:\n",
    "- Две подсети копии `Актёр-тренер` и `Актёр-цель`\n",
    "- Две дублирующие подсети `Критик-тренер`\n",
    "- Две подсети `Критик-цель`\n",
    "- Коэффициент `Альфа-регулятор`.\n",
    "\n",
    "### Две подсети копии `Актёр-тренер` и `Актёр-цель`\n",
    "`Актёр-цель` используется исключительно для возможности распараллеливать обучение и заполнение `Буфера повтора` новыми данными и является полной копией нейросети `Актёр-тренер`.\n",
    "\n",
    "### Две дублирующие подсети `Критик-тренер`\n",
    "Необходимы для минимизации ошибки.\n",
    "\n",
    "### Две подсети `Критик-цель`\n",
    "Используются для плавного обучения методом `скользящее среднее`.\n",
    "\n",
    "### Коэффициент `Альфа-регулятор`\n",
    "Исполняет роль микроподстройки процесса обучения, для увеличения точности.\n",
    "\n",
    "## Блок `Игрок`\n",
    "Существует некая среда, в которой необходимо производить определённые действия для достижения поставленной цели. Для упрощения понимания, назовём среду `Игрой`. Задача `Игрока` собирать данные наблюдения от `Игры`, совершать действия, и получать от `Игры` `Награду` или производить самостоятельно `Оценку` этих действий. Каждое совершённое действие, это ход. За один ход мы имеем следующий набор данных: `Прошлое наблюдение`, `Текущее наблюдение`, `Совершённое действие`, `Награда` или `Оценка`, `Cтатус конца`. Решение, о том, какое совершать действие принимает сеть `Актёр-цель` на основе данных `Прошлого наблюдения`. Если принятое решение приводит к ситуации, которую можно считать концом, то `Игрок` завершает и сбрасывает `Игру`.\n",
    "\n",
    "Оценка действий бывает двух типов:\n",
    "- Оценка награды за каждый шаг\n",
    "- Оценка всего эпизода\n",
    "\n",
    "Каждый ход записывается в `Буфер повтора` для дальнейшего обучения и называется `Траекторией`.\n",
    "\n",
    "### Оценка награды за каждый шаг\n",
    "`Игрок` совершает действие и сразу производит запись в `Буфер повтора` следующие показатели: `Прошлое наблюдение`, `Текущее наблюдение`, `Cовершённое действие`, `Награда`. Тогда `Оценку` производит `Тренер`.\n",
    "\n",
    "### Оценка всего эпизода\n",
    "`Игрок` совершает действия и производит запись во `Временный буфер`. По окончании эпизода рассчитывает `Оценку` на каждом ходу и после производит запись в `Буфер повтора` всего эпизода со следующими показателями: `Прошлое наблюдение`, `Текущее наблюдение`, `Cовершённое действие`, `Оценка`.\n",
    "\n",
    "## Блок `Генетический буфер повтора`\n",
    "Представляет из себя циклический `Буфер повтора`, который при переполнении начинает перезатирать более старые данные более новыми. А также включает в себя `Древовидный буфер суммы` и `Древовидный буфер максимума`, используемые для расчёта приоритета каждого хода хранящегося в `Буфере повтора`. Древовидные буферы в паре помогают реализовать подобие генетического алгоритма при выборе данных из `Буфера повтора`, что позволяет сильно ускорить процесс обучения, а также уменьшает вероятность сбивания или зависания обучаемой модели в плохом состоянии. Плохое состояние может быть следствием привыкания нейросети к плохим результатам.\n",
    "\n",
    "## Блок `Тренер`\n",
    "Основной мозговой центр алгоритма, который управляет всеми остальными блоками.\n",
    "\n",
    "Цикл обучения на каждый шаг `Игрока`:\n",
    "1. Выбрать блок `Траекторий` из `Генетический буфера повтора` с учётом приоритетов.\n",
    "2. Обучить независимо два `Критик-тренера`.\n",
    "3. Обновить `Критик-цели` методом `скользящее среднее`.\n",
    "4. Обучить `Актёр-тренер` и `Альфа-регулятор`.\n",
    "5. Обновить `Актёр-цель`.\n",
    "6. Обновить приоритеты в `Генетическом буфере повтора` для обработанных ходов из выбранного блока.\n",
    "\n",
    "Процесс обучения стандартный: прямое распространение, вычисление потери, вычисление градиентов, обратное распространение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример результатов тренировки `LunarLander-v2`\n",
    "<table style=\"float:left;\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: center;\">Средний счёт за эпизод</td>\n",
    "        <td style=\"text-align: center;\">Среднее число шагов за эпизод</td>\n",
    "    </tr><tr>\n",
    "        <td><img src=\"GSAC-Score.svg\" width=\"320pt\"></td>\n",
    "        <td><img src=\"GSAC-Steps.svg\" width=\"320pt\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:24.757385Z",
     "start_time": "2020-06-29T17:51:24.753978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Использовать GPU?\n",
    "bGPU = False\n",
    "# Размер одного слоя нейросети `Кодер`\n",
    "uEncoderLayerSize = 64\n",
    "# Имя модели и логов\n",
    "sName = \"%d\" % uEncoderLayerSize\n",
    "# Максимальное количество шагов в эпизоде\n",
    "uEpisodeStepLimit = 1024\n",
    "# Размер буфера повтора, должен вмещать хотябы 100 эпизодов\n",
    "uReplayCapacity = 128 * 1024\n",
    "# Размер пакета данных при обучении\n",
    "uBatchSize = 256\n",
    "# Восстановить граф из сохранения?\n",
    "bRestore = False\n",
    "# Производить вывод на экран?\n",
    "bRender = False\n",
    "# Производить оценку всего эпизода?\n",
    "bEpisodeRating = False\n",
    "# Делать выборку из буфера повтора с учётом приоритетов?\n",
    "bPriorityMode = False\n",
    "# Фактор забывания оценки\n",
    "fRatingDiscountFactor = 0.99\n",
    "# Коэфициент обновления целевой нейронной сети\n",
    "fTrainableTargetAverageUpdateCoef = 0.005\n",
    "# Уровень логирования и ведения статистики\n",
    "uLogLevel = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка рабочего пространства\n",
    "Подключение модулей и их настройка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:37.187031Z",
     "start_time": "2020-06-29T17:51:25.911838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Отключаем спам в консоль от Tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "# Отключаем работу с GPU для маленьких сетей, т.к. CPU работает быстрее\n",
    "if not bGPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Математика\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow._api.v2.compat.v1 as tf1\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.initializers as ki\n",
    "\n",
    "# Отключить выделение всей доступной памяти\n",
    "if bGPU:\n",
    "    aoGPUPhysicalList = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if aoGPUPhysicalList:\n",
    "        try:\n",
    "            for oGPUDevice in aoGPUPhysicalList:\n",
    "                tf.config.experimental.set_memory_growth(oGPUDevice, True)\n",
    "        except RuntimeError as e:\n",
    "            pass\n",
    "\n",
    "# Указываем тип данных по умолчанию для сетей keras\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# Ручной контроль создания графов и управления сессиями, чтобы убирать проблему утечки памяти\n",
    "tf1.disable_eager_execution()\n",
    "\n",
    "# Дата и время\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Виртуальное игровое окружение для `Обучения с подкреплением`\n",
    "import gym\n",
    "\n",
    "# Поддержка абтрактных методов\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:37.396745Z",
     "start_time": "2020-06-29T17:51:37.241120Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Получить информацию об использовании памяти текущим процессом\n",
    "def getMemoryUsage():\n",
    "    oProcess = psutil.Process(os.getpid())\n",
    "    return \"%d:%d\" % (oProcess.memory_info().rss, oProcess.memory_info().vms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Надстройка Tensorflow\n",
    "Набор функций и классов упрощающих ручную работу с графами и сессиями _Tensorflow_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:40.269019Z",
     "start_time": "2020-06-29T17:51:40.236395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Типы по умолчанию\n",
    "tfFloat = tf.keras.backend.floatx()\n",
    "tfInt = tf.int32\n",
    "\n",
    "# Инициализатор массивов переменных в графе (для внутреннего использования)\n",
    "class ArrayInitializer(ki.Initializer):\n",
    "    def __init__(self, aValue):\n",
    "        super(ArrayInitializer, self).__init__()\n",
    "        self.aValue = aValue\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return tf.convert_to_tensor(self.aValue, dtype=dtype)\n",
    "\n",
    "# Создать инициализатор в зависимости от формы и значения (для внутреннего использования)\n",
    "def tfInitializer(tShape, oValue=None):\n",
    "    if (oValue is not None) and (not isinstance(oValue, ki.Initializer)):\n",
    "        if isinstance(oValue, list):\n",
    "            oValue = np.array(oValue)\n",
    "        if type(oValue) is np.ndarray:\n",
    "            if oValue.shape != tShape:\n",
    "                raise TypeError(\"Неверный размер инициализатора %s, вместо %s\" % (oValue.shape, tShape))\n",
    "            return ArrayInitializer(oValue)\n",
    "        elif oValue == 0:\n",
    "            return ki.Zeros()\n",
    "        elif oValue == 1:\n",
    "            return ki.Ones()\n",
    "        else:\n",
    "            return ki.Constant(oValue)\n",
    "    return oValue\n",
    "\n",
    "# Объявить локальную переменную, используемую внутри одного функционального блока графа\n",
    "def tfLocalVariable(sName, oType, oValue=None, bTrainable=False):\n",
    "    if isinstance(oType, tuple):\n",
    "        sType, tShape = oType[0], oType[1]\n",
    "    else:\n",
    "        sType, tShape = oType, []\n",
    "    oValue = tfInitializer(tShape, oValue)\n",
    "    return tf1.get_local_variable(sName, shape=tShape, dtype=sType, initializer=oValue, trainable=bTrainable, use_resource=True)\n",
    "\n",
    "# Объявить глобальную переменную, используемую всем графом в течении сессии\n",
    "def tfGlobalVariable(sName, oType, oValue=None, bTrainable=False):\n",
    "    if isinstance(oType, tuple):\n",
    "        sType, tShape = oType[0], oType[1]\n",
    "    else:\n",
    "        sType, tShape = oType, []\n",
    "    oValue = tfInitializer(tShape, oValue)\n",
    "    return tf1.get_variable(sName, shape=tShape, dtype=sType, initializer=oValue, trainable=bTrainable, use_resource=True)\n",
    "\n",
    "# Объявить константу\n",
    "def tfConstant(sName, oType, oValue):\n",
    "    sType = oType[0] if isinstance(oType, tuple) else oType\n",
    "    return tf1.constant(oValue, dtype=sType, name=sName)\n",
    "\n",
    "# Объявить узел входных данных графа\n",
    "def tfInput(sName, oType):\n",
    "    if isinstance(oType, tuple):\n",
    "        sType, tShape = oType[0], oType[1]\n",
    "    else:\n",
    "        sType, tShape = oType, []\n",
    "    return tf1.placeholder(sType, shape=tShape, name=sName)\n",
    "\n",
    "# Псевдоним сокращение для функции ожидания вычисления набора операций\n",
    "class tfWait(object):\n",
    "    def __init__(self, aDependencies):\n",
    "        self.tfControl = tf.control_dependencies(aDependencies)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.tfControl.__enter__()\n",
    "\n",
    "    def __exit__(self, clsErrorType, errValue, oTraceback):\n",
    "        return self.tfControl.__exit__(clsErrorType, errValue, oTraceback)\n",
    "\n",
    "# Текущая активная сессия\n",
    "tfActiveSession = None\n",
    "\n",
    "# Создать сессию для управления выполнением графа\n",
    "class tfSession(object):\n",
    "    def __init__(self, tfoGraph=None, sTarget=''):\n",
    "        self.tfoSession = tf1.Session(target=sTarget, graph=tfoGraph.tfoGraph)\n",
    "\n",
    "    def __enter__(self):\n",
    "        global tfActiveSession\n",
    "        self.tfoDefault = self.tfoSession.as_default()\n",
    "        self.tfoDefault.__enter__()\n",
    "        self.tfRestoreSession = tfActiveSession\n",
    "        tfActiveSession = self\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, clsErrorType, errValue, oTraceback):\n",
    "        global tfActiveSession\n",
    "        self.tfoSession.close()\n",
    "        self.tfoDefault = None\n",
    "        tfActiveSession = self.tfRestoreSession\n",
    "\n",
    "    def initGlobal(self):\n",
    "        self.tfoSession.run(tf1.global_variables_initializer())\n",
    "\n",
    "    def initLocal(self):\n",
    "        self.tfoSession.run(tf1.local_variables_initializer())\n",
    "\n",
    "    def eval(self, tfoOutputTensor, oInputDictionary=None):\n",
    "        return self.tfoSession.run(tfoOutputTensor, oInputDictionary)\n",
    "\n",
    "# Вычислить результат в узле графа внутри текущей сессии\n",
    "def tfEval(tfoOutputTensor, oInputDictionary=None):\n",
    "    return tfActiveSession.eval(tfoOutputTensor, oInputDictionary)\n",
    "\n",
    "# Произвести инициализацию глобальных переменных внутри текущей сессии\n",
    "def tfInitGlobal():\n",
    "    tfActiveSession.initGlobal()\n",
    "\n",
    "# Произвести инициализацию локальных переменных внутри текущей сессии\n",
    "def tfInitLocal():\n",
    "    tfActiveSession.initLocal()\n",
    "\n",
    "# Текущий активный граф\n",
    "tfActiveGraph = None\n",
    "\n",
    "# Создать граф\n",
    "class tfGraph(object):\n",
    "    def __init__(self):\n",
    "        self.tfoGraph = tf.Graph()\n",
    "        self.tfoDefault = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        global tfActiveGraph\n",
    "        self.tfoDefault = self.tfoGraph.as_default()\n",
    "        self.tfoDefault.__enter__()\n",
    "        self.tfRestoreGraph = tfActiveGraph\n",
    "        tfActiveGraph = self\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, clsErrorType, errValue, oTraceback):\n",
    "        global tfActiveGraph\n",
    "        self.tfoDefault = None\n",
    "        tfActiveGraph = self.tfRestoreGraph\n",
    "        \n",
    "# Псевдоним сокращение для оптимизатора\n",
    "class AMSgrad(ko.Adam):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super(AMSgrad, self).__init__(lr=lr, amsgrad=True)\n",
    "\n",
    "# Ограничение значений градиентов для предотвращения `inf` и `nan`\n",
    "def fnClipGradients(aGradients, tfMaxNormal):\n",
    "    aClippedGradients = []\n",
    "    for tfoGrad in aGradients:\n",
    "        if tfoGrad is not None:\n",
    "            if isinstance(tfoGrad, tf.IndexedSlices):\n",
    "                tfoTemp = tf.clip_by_norm(tfoGrad.values, tfMaxNormal)\n",
    "                tfoGrad = tf.IndexedSlices(tfoTemp, tfoGrad.indices, tfoGrad.dense_shape)\n",
    "            else:\n",
    "                tfoGrad = tf.clip_by_norm(tfoGrad, tfMaxNormal)\n",
    "        aClippedGradients.append(tfoGrad)\n",
    "    return aClippedGradients\n",
    "\n",
    "# Копирование весов из одной нейросети в другую\n",
    "def fnHardUpdate(tafTargetVariables, tafSourceVariables):\n",
    "    atopUpdates = []\n",
    "    tfStrategy = tf.distribute.get_strategy()\n",
    "\n",
    "    for (tfoTarget, tfoSource) in zip(tafTargetVariables, tafSourceVariables):\n",
    "        def fnUpdate(tfoTarget, tfoSource):\n",
    "            return tfoTarget.assign(tfoSource)\n",
    "\n",
    "        if tf.distribute.has_strategy() and tfoTarget.trainable:\n",
    "            topUpdate = tfStrategy.extended.update(tfoTarget, fnUpdate, args=(tfoSource,))\n",
    "        else:\n",
    "            topUpdate = fnUpdate(tfoTarget, tfoSource)\n",
    "\n",
    "        atopUpdates.append(topUpdate)\n",
    "    return tf.group(*atopUpdates)\n",
    "\n",
    "# Наложение весов из одной нейросети в другую методом `скользящее среднее`\n",
    "def fnSoftUpdate(tafTargetVariables, tafSourceVariables, tfZero, tfOne, tfTrainableTargetAverageForgetCoef, tfTrainableTargetAverageUpdateCoef):\n",
    "    atopUpdates = []\n",
    "    tfStrategy = tf.distribute.get_strategy()\n",
    "\n",
    "    for (tfoTarget, tfoSource) in zip(tafTargetVariables, tafSourceVariables):\n",
    "        def fnUpdate(tfoTarget, tfoSource):\n",
    "            if not tfoTarget.trainable:\n",
    "                tfTargetAverageForgetCoef = tfZero\n",
    "                tfTargetAverageUpdateCoef = tfOne\n",
    "            else:\n",
    "                tfTargetAverageForgetCoef = tfTrainableTargetAverageForgetCoef\n",
    "                tfTargetAverageUpdateCoef = tfTrainableTargetAverageUpdateCoef\n",
    "\n",
    "            return tfoTarget.assign(tfoTarget * tfTargetAverageForgetCoef + tfoSource * tfTargetAverageUpdateCoef)\n",
    "\n",
    "        if tf.distribute.has_strategy() and tfoTarget.trainable:\n",
    "            topUpdate = tfStrategy.extended.update(tfoTarget, fnUpdate, args=(tfoSource,))\n",
    "        else:\n",
    "            topUpdate = fnUpdate(tfoTarget, tfoSource)\n",
    "\n",
    "        atopUpdates.append(topUpdate)\n",
    "    return tf.group(*atopUpdates)\n",
    "\n",
    "# Записать детальную статистику тензора, одной переменной графа\n",
    "def fnTensorSummary(oSummaryWriter, sTag, tfoVariable, tuStep):\n",
    "    with oSummaryWriter.as_default():\n",
    "        with tf.name_scope(sTag):\n",
    "            return tf.group(\n",
    "                tf.summary.histogram('Histogram', tfoVariable, tuStep),\n",
    "                tf.summary.scalar('Mean', tf.reduce_mean(tfoVariable, 'fMean'), tuStep),\n",
    "                tf.summary.scalar('MeanAbs', tf.reduce_mean(tf.abs(tfoVariable), 'fMeanAbs'), tuStep),\n",
    "                tf.summary.scalar('Max', tf.reduce_max(tfoVariable), tuStep),\n",
    "                tf.summary.scalar('Min', tf.reduce_min(tfoVariable), tuStep)\n",
    "            )\n",
    "\n",
    "# Записать детальную статистику весовых коэфициентов нейросети\n",
    "def fnWeightsSummary(oSummaryWriter, zipWeightfoGradientsAndVariables, tuStep):\n",
    "    aOps = []\n",
    "    with oSummaryWriter.as_default():\n",
    "        for tfaGradientsGroup, tfaVariablesGroup in zipWeightfoGradientsAndVariables:\n",
    "            sGroupName = tfaVariablesGroup.name.replace(':', '_')\n",
    "\n",
    "            if isinstance(tfaVariablesGroup, tf.IndexedSlices):\n",
    "                tfaValues = tfaVariablesGroup.values\n",
    "            else:\n",
    "                tfaValues = tfaVariablesGroup\n",
    "            aOps.append(tf.summary.histogram('Weights/' + sGroupName, tfaValues, tuStep))\n",
    "            aOps.append(tf.summary.scalar('WeightsNorm/' + sGroupName, tf.linalg.global_norm([tfaValues]), tuStep))\n",
    "\n",
    "            if tfaGradientsGroup is not None:\n",
    "                if isinstance(tfaGradientsGroup, tf.IndexedSlices):\n",
    "                    tfaGradients = tfaGradientsGroup.values\n",
    "                else:\n",
    "                    tfaGradients = tfaGradientsGroup\n",
    "                aOps.append(tf.summary.histogram('Gradients/' + sGroupName, tfaGradients, tuStep))\n",
    "                aOps.append(tf.summary.scalar('GradientsNorm/' + sGroupName, tf.linalg.global_norm([tfaGradients]), tuStep))\n",
    "    return tf.group(*aOps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Набор функций для выбора действия в дискретных моделях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:41.687319Z",
     "start_time": "2020-06-29T17:51:41.680512Z"
    }
   },
   "outputs": [],
   "source": [
    "def fnSelectBest(tafUnnormalizedLogProbabilities):\n",
    "    return tf.argmax(tafUnnormalizedLogProbabilities, axis=-1, output_type=tfInt)\n",
    "\n",
    "def fnSelectRandom(tafUnnormalizedLogProbabilities):\n",
    "    return tf.squeeze(tf.random.categorical(tafUnnormalizedLogProbabilities, 1, dtype=tfInt), axis=-1)\n",
    "\n",
    "def fnSelectNoisyBest(tafUnnormalizedLogProbabilities):\n",
    "    with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "        tfTotalMinLogInput = tfConstant('fMinLogInput', tfFloat, 1e-8)\n",
    "        tfTotalMaxLogInput = tfConstant('fMaxLogInput', tfFloat, 1-1e-8)\n",
    "\n",
    "    with tf.name_scope('fnSelectNoisyBest'):\n",
    "        # Массив случайных чисел для добавления шума к логарифмическим вероятностям\n",
    "        tafRandomUniform = tf.random.uniform(tafUnnormalizedLogProbabilities.shape, minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "        # Распределение Гумбеля для случайных чисел\n",
    "        tafGumbel = -tf.math.log(-tf.math.log(tafRandomUniform)) # pylint: disable=invalid-unary-operand-type\n",
    "        # Ненормализованные предполагаемые логарифмические вероятности возможных действий с добавлением шума\n",
    "        tafUnnormalizedNoisyLogProbabilities = tafUnnormalizedLogProbabilities + tafGumbel\n",
    "\n",
    "    return tf.argmax(tafUnnormalizedNoisyLogProbabilities, axis=-1, output_type=tfInt)\n",
    "\n",
    "def fnSelectNoisyRandom(tafUnnormalizedLogProbabilities):\n",
    "    with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "        tfTotalMinLogInput = tfConstant('fMinLogInput', tfFloat, 1e-8)\n",
    "        tfTotalMaxLogInput = tfConstant('fMaxLogInput', tfFloat, 1-1e-8)\n",
    "\n",
    "    with tf.name_scope('fnSelectNoisyRandom'):\n",
    "        # Массив случайных чисел для добавления шума к логарифмическим вероятностям\n",
    "        tafRandomUniform = tf.random.uniform(tafUnnormalizedLogProbabilities.shape, minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "        # Распределение Гумбеля для случайных чисел\n",
    "        tafGumbel = -tf.math.log(-tf.math.log(tafRandomUniform)) # pylint: disable=invalid-unary-operand-type\n",
    "        # Ненормализованные предполагаемые логарифмические вероятности возможных действий с добавлением шума\n",
    "        tafUnnormalizedNoisyLogProbabilities = tafUnnormalizedLogProbabilities + tafGumbel\n",
    "\n",
    "    return tf.squeeze(tf.random.categorical(tafUnnormalizedNoisyLogProbabilities, 1, dtype=tfInt), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель среды\n",
    "Модель среды определяет текущее состояние окружения, и на основе команд управления, вычисляет следующее состояние окружения, награду за совершённое действие, а также информирует о конце, т.е. необходимости сброса среды в начальное состояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:42.965560Z",
     "start_time": "2020-06-29T17:51:42.962374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Базовый класс среды\n",
    "class EnvironmentImpl(object):\n",
    "    def __init__(self, uObservationSize, uActionsSize, bDiscrete):\n",
    "        # Размер массива данных о состоянии наблюдения\n",
    "        self.uObservationSize = uObservationSize\n",
    "        # Размер массива действий\n",
    "        self.uActionsSize = uActionsSize\n",
    "        # Дискретный или непрерывный тип управления средой\n",
    "        self.bDiscrete = bDiscrete\n",
    "\n",
    "    # Сброс среды. Возвращает текущее состояние наблюдения\n",
    "    @abc.abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    # Следующий шаг состояния наблюдения. Возвращает новое состояние наблюдения, награду и состояние завершения\n",
    "    @abc.abstractmethod\n",
    "    def step(self, oAction):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:44.209273Z",
     "start_time": "2020-06-29T17:51:44.202193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Модель среды для игры LunarLander\n",
    "class CustomEnvironment(EnvironmentImpl):\n",
    "    def __init__(self, bRender=True):\n",
    "        sEnvironment = 'LunarLander-v2'\n",
    "        self.oEnvironment = gym.make(sEnvironment)\n",
    "        \n",
    "        # Ограничение максимальной длительности игры\n",
    "        # Должно быть хотябы в 4 раза больше предполагаемой средней длительности, но не слишком большым\n",
    "        self.oEnvironment._max_episode_steps = uEpisodeStepLimit * 10\n",
    "        \n",
    "        tObservationShape = self.oEnvironment.observation_space.shape\n",
    "        tActionsShape = (self.oEnvironment.action_space.n,)\n",
    "\n",
    "        super(CustomEnvironment, self).__init__(\n",
    "            np.prod(list(tObservationShape)),\n",
    "            np.prod(list(tActionsShape)),\n",
    "            True\n",
    "        )\n",
    "\n",
    "        self.bRender = bRender\n",
    "\n",
    "    def reset(self):\n",
    "        return self.oEnvironment.reset()\n",
    "\n",
    "    def step(self, uAction):\n",
    "        aObservation, fReward, bDone, _ = self.oEnvironment.step(uAction)\n",
    "\n",
    "        if self.bRender:\n",
    "            self.oEnvironment.render()\n",
    "\n",
    "        oInfo = {}\n",
    "        return aObservation, fReward, bDone, oInfo\n",
    "\n",
    "    def close(self):\n",
    "        self.oEnvironment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели нейросетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:46.093768Z",
     "start_time": "2020-06-29T17:51:46.079441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Базовая модель нейросети\n",
    "class ModelImpl(tf.keras.Model):\n",
    "    def __init__(self, sName=None):\n",
    "        super(ModelImpl, self).__init__(name=sName)\n",
    "\n",
    "    # Переопределение метода для того, чтобы Tensorflow объявил переменную `self._build_input_shape`\n",
    "    #  `self._build_input_shape` используетя для логирования структуры модели нейросети\n",
    "    def build(self, tInputShape):\n",
    "        super(ModelImpl, self).build(tInputShape)\n",
    "\n",
    "# ВАЖНО! Не использовать `relu`, т.к. это приводит к проблеме `исчезающего градиента`, близкого или равного нулю\n",
    "\n",
    "# Нейросеть `Критик`, производит оценку возможной выгоды окружения среды при совершении определённых действий\n",
    "class CriticNetwork(ModelImpl):\n",
    "    def __init__(self, sName=None):\n",
    "        super(CriticNetwork, self).__init__(sName=sName)\n",
    "        # Блок `Выпрямитель`\n",
    "        self.fnFlattenObservations = kl.Flatten()\n",
    "        self.fnFlattenActions = kl.Flatten()\n",
    "        self.fnConcatInput = kl.Concatenate()\n",
    "        # Блок `Кодер`\n",
    "        self.fnEncoder = tf.keras.Sequential()\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(32, activation='elu'))\n",
    "        # Блок `Критик`\n",
    "        self.fnCritic = kl.Dense(1, activation='linear')\n",
    "\n",
    "    # Создать переменные весов для нейросети\n",
    "    def build(self, tObservationShape, tActionsShape):\n",
    "        super(CriticNetwork, self).build((tObservationShape[0], np.prod(list(tObservationShape[1:])) + np.prod(list(tActionsShape[1:]))))\n",
    "\n",
    "    # Подготовить данные с помощью блока `Выпрямитель` для вычисления результата нейросети\n",
    "    def prepare(self, tafObservations, tafActions):\n",
    "        tafFlatObservations = self.fnFlattenObservations.call(tafObservations)\n",
    "        tafFlatActions = self.fnFlattenActions.call(tafActions)\n",
    "        tafStates = self.fnConcatInput([tafFlatObservations, tafFlatActions])\n",
    "        return tafStates\n",
    "\n",
    "    # Вычислить результат нейросети (предсказать рейтинг)\n",
    "    def call(self, tafStates):\n",
    "        tafEncoded = self.fnEncoder(tafStates)\n",
    "        tafPredRating = self.fnCritic(tafEncoded)\n",
    "        return tafPredRating\n",
    "\n",
    "# Нейросеть `Дискретный актёр`, генерирует ненормализованные логарифмические вероятности действий\n",
    "class DiscreteActorNetwork(tf.keras.Sequential):\n",
    "    def __init__(self, uActionCount, sName=None):\n",
    "        super(DiscreteActorNetwork, self).__init__(name=sName)\n",
    "        # Блок `Кодер`\n",
    "        self.fnEncoder = tf.keras.Sequential()\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(32, activation='elu'))\n",
    "        # Блок `Актёр`\n",
    "        self.fnAction = kl.Dense(uActionCount, activation='linear')\n",
    "        # Последовательность вычислений\n",
    "        self.add(self.fnEncoder)\n",
    "        self.add(self.fnAction)\n",
    "\n",
    "# Нейросеть `Непрерывный актёр`, генерирует среднее и стандартное отклонение возможного действия\n",
    "class ContinuousActorNetwork(ModelImpl):\n",
    "    def __init__(self, uActionCount, sName=None):\n",
    "        super(ContinuousActorNetwork, self).__init__(sName=sName)\n",
    "        # Блок `Кодер`\n",
    "        self.fnEncoder = tf.keras.Sequential()\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(32, activation='elu'))\n",
    "        # Блок `Актёр`\n",
    "        # Начальные значения устанавливаются в районе 0 с небольшим отклонением, для избежания резких и непредвиденных действий\n",
    "        self.fnMean = kl.Dense(uActionCount, activation='linear',\n",
    "            kernel_initializer=ki.VarianceScaling(0.1),\n",
    "            bias_initializer=ki.Zeros(),)\n",
    "        self.fnStd = kl.Dense(uActionCount, activation='linear',\n",
    "            kernel_initializer=ki.VarianceScaling(0.1),\n",
    "            bias_initializer=ki.Constant(0.0),)\n",
    "\n",
    "    # Создать переменные весов для нейросети\n",
    "    def build(self, tObservationShape):\n",
    "        super(ContinuousActorNetwork, self).build(tObservationShape)\n",
    "\n",
    "    # Вычисление результата нейросети\n",
    "    def call(self, tafState):\n",
    "        tafEncoded = self.fnEncoder(tafState)\n",
    "        tafPredLocation = self.fnMean(tafEncoded)\n",
    "        tafPredScale = self.fnStd(tafEncoded)\n",
    "        return tafPredLocation, tafPredScale\n",
    "\n",
    "# Создать нейросеть `Актёр` в зависимости от типа среды\n",
    "def CreateActor(oEnvironment, uBatchSize=256, sName='Actor'):\n",
    "    if oEnvironment.bDiscrete:\n",
    "        nnActor = DiscreteActorNetwork(oEnvironment.uActionsSize, sName)\n",
    "    else:\n",
    "        nnActor = ContinuousActorNetwork(oEnvironment.uActionsSize, sName)\n",
    "    nnActor.build((uBatchSize, oEnvironment.uObservationSize))\n",
    "    return nnActor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Управление средой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:47.555587Z",
     "start_time": "2020-06-29T17:51:47.544253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Класс управления `Игрок`, используя нейросеть `Актёр` совершает дейтвия в виртуальной среде\n",
    "# fnPolicy - функция преобразующая логарифмические вероятности в номер действия\n",
    "class CustomPlayer(object):\n",
    "    def __init__(self, oEnvironment, nnActor, fnPolicy=fnSelectBest):\n",
    "        self.oEnvironment = oEnvironment\n",
    "        self.nnActor = nnActor\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        # Константы необходимые для работы с графом\n",
    "        with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "            tuActionsSize = tfConstant('uActionsSize', tfInt, self.oEnvironment.uActionsSize)\n",
    "            tuZero = tfConstant('uZero', tfInt, 0)\n",
    "\n",
    "        # Формирование блока графа для вычисления совершаемого действия\n",
    "        with tf.name_scope('CustomPlayer'):\n",
    "            # Функциональный блок `fnAction`\n",
    "            with tf.name_scope('fnAction'):\n",
    "                with tf1.variable_scope('Input', reuse=tf1.AUTO_REUSE):\n",
    "                    # Узел входных данных наблюдения за средой\n",
    "                    self.tinafReplayObservation = tfInput('afObservation', (tfFloat, [self.oEnvironment.uObservationSize]))\n",
    "\n",
    "                # Вычислить ненормализованные логарифмические вероятности следующего действия\n",
    "                tafActions = nnActor.call(self.tinafReplayObservation[None, :], training=False)\n",
    "                self.tafActions = tf.squeeze(tafActions)\n",
    "                # Преобразовать вероятности в номер действия используя функцию политики\n",
    "                self.tuAction = tf.squeeze(tf.clip_by_value(fnPolicy(tafActions), tuZero, tuActionsSize))\n",
    "\n",
    "    # Вычислить следующее действие\n",
    "    def action(self, afObservation):\n",
    "        return tfEval([self.tafActions, self.tuAction], {self.tinafReplayObservation: afObservation})\n",
    "\n",
    "    # Сброс среды и состояния окружения\n",
    "    def reset(self):\n",
    "        afObservation = self.oEnvironment.reset()\n",
    "        self.afPrevObservation = None\n",
    "        self.uAction = None\n",
    "        self.afActions = None\n",
    "        self.afObservation = np.array(afObservation, dtype=tfFloat).flatten()\n",
    "        self.uStep = 0\n",
    "        self.fScore = 0\n",
    "        self.fReward = 0\n",
    "        self.fAverageReward = 0\n",
    "        self.bDone = False\n",
    "\n",
    "    # Совершить следующее предполагаемое действие\n",
    "    def next(self):\n",
    "        if not self.bDone:\n",
    "            self.afActions, self.uAction = self.action(self.afObservation)\n",
    "\n",
    "            if self.uStep >= uEpisodeStepLimit:\n",
    "                self.uAction = 0\n",
    "            \n",
    "            afObservation, self.fReward, bDone, _ = self.oEnvironment.step(self.uAction)\n",
    "\n",
    "            self.afPrevObservation = self.afObservation\n",
    "            self.afObservation = np.array(afObservation, dtype=tfFloat).flatten()\n",
    "            self.uStep += 1\n",
    "            self.fScore += self.fReward\n",
    "            self.fAverageReward = self.fAverageReward * 0.999 + self.fReward * 0.001\n",
    "            if bDone:\n",
    "                self.bDone = True\n",
    "        return self.bDone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Агент `Soft Actor Critic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:50.976829Z",
     "start_time": "2020-06-29T17:51:50.803624Z"
    }
   },
   "outputs": [],
   "source": [
    "class SacAgent(object):\n",
    "    def __init__(self,\n",
    "            oPlayer,\n",
    "            uReplayCapacity=128*1024,\n",
    "            fRatingDiscountFactor=0.99,\n",
    "            uBatchSize=256,\n",
    "            bEpisodeRating=False,\n",
    "            bPriorityMode=True,\n",
    "            fTrainableTargetAverageUpdateCoef=0.005,\n",
    "            tfTrainStepCounter=None,\n",
    "            uLogLevel=1,\n",
    "            sLogsPath='logs/',\n",
    "            sRestorePath='models/'):\n",
    "\n",
    "        if uReplayCapacity < 2:\n",
    "            raise TypeError('uReplayCapacity должно быть больше 1, а задано %d' % uReplayCapacity)\n",
    "\n",
    "        # Входные параметры\n",
    "\n",
    "        self.oPlayer = oPlayer\n",
    "        self.uBatchSize = uBatchSize\n",
    "        self.bEpisodeRating = bEpisodeRating\n",
    "        self.bPriorityMode = bPriorityMode\n",
    "        self.fRatingDiscountFactor = fRatingDiscountFactor\n",
    "\n",
    "        # Коэфициент сглаживания статистики среднего значния градиентов\n",
    "        fGradientNormalUpdateCoef = 0.01\n",
    "        # Максимальное среднее значение градиентов\n",
    "        fMaxGradientNormal = 200\n",
    "\n",
    "        self.uLogLevel = uLogLevel\n",
    "\n",
    "        # Оптимизаторы сетей\n",
    "\n",
    "        self.koActorOptimizer = AMSgrad(3e-4)\n",
    "        self.koCriticOptimizer = AMSgrad(3e-4)\n",
    "        self.koAlphaOptimizer = AMSgrad(3e-4)\n",
    "\n",
    "        # Самописец статистики\n",
    "\n",
    "        dtCurrentTime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        sLogPath = sLogsPath + dtCurrentTime\n",
    "        self.oSummaryWriter = tf.summary.create_file_writer(sLogPath)\n",
    "\n",
    "        # Временное хранилище ходов для целого эпизода\n",
    "\n",
    "        if self.bEpisodeRating:\n",
    "            # Текущий размер данных во временном буфере\n",
    "            self.uBufferSize = 0\n",
    "            # Текущая вместительность временного буфера\n",
    "            self.uBufferCapacity = 256\n",
    "\n",
    "            # Хранилище временного буфера: наблюдения, действия, награды\n",
    "            self.afPrevObservations = np.zeros((self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize), dtype=tfFloat)\n",
    "            self.afObservations = np.zeros((self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize), dtype=tfFloat)\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                self.auActions = np.zeros((self.uBufferCapacity,), dtype=np.int32)\n",
    "            else:\n",
    "                self.afActions = np.zeros((self.uBufferCapacity, self.oPlayer.oEnvironment.uActionsSize), dtype=tfFloat)\n",
    "            self.afRewards = np.zeros((self.uBufferCapacity,), dtype=tfFloat)\n",
    "            self.afRatings = np.zeros((self.uBufferCapacity,), dtype=tfFloat)\n",
    "\n",
    "        # Списки переменных для сохранения на диск\n",
    "\n",
    "        aTrainVariables = []\n",
    "        aTargetVariables = oPlayer.nnActor.trainable_variables\n",
    "        aReplayBufferVariables = []\n",
    "\n",
    "        # Глобальные константы графа\n",
    "\n",
    "        with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "            tfZero = tfConstant('fZero', tfFloat, 0)\n",
    "            tfHalf = tfConstant('fHalf', tfFloat, 0.5)\n",
    "            tfOne = tfConstant('fOne', tfFloat, 1)\n",
    "            tuOne = tfConstant('uOne', tfInt, 1)\n",
    "            if self.bPriorityMode:\n",
    "                tuTwo = tfConstant('uTwo', tfInt, 2)\n",
    "\n",
    "            tuActionsSize = tfConstant('uActionsSize', tfInt, oEnvironment.uActionsSize)\n",
    "            tuOne64 = tfConstant('uOne64', tf.int64, 1)\n",
    "\n",
    "            if fMaxGradientNormal is not None:\n",
    "                tfMaxGradientNormal = tfConstant('fMaxGradientNormal', tfFloat, fMaxGradientNormal)\n",
    "\n",
    "            if self.uLogLevel > 1:\n",
    "                tfGradientNormalUpdateCoef = tfConstant('fGradientNormalUpdateCoef', tfFloat, fGradientNormalUpdateCoef)\n",
    "                tfGradientNormalForgetCoef = tfConstant('fGradientNormalForgetCoef', tfFloat, 1.0 - fGradientNormalUpdateCoef)\n",
    "\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                tfTotalMinLogInput = tfConstant('fMinClip', tfFloat, 1e-8)\n",
    "                tfTotalMaxLogInput = tfConstant('fMaxClip', tfFloat, 1-1e-8)\n",
    "            else:\n",
    "                tfLogSqrtPi2 = tfConstant('fLogSqrtPi2', tfFloat, math.log(math.sqrt(math.pi * 2.0)))\n",
    "                tfTotalMinScale = tfConstant('fMinScale', tfFloat, -20)\n",
    "                tfTotalMaxScale = tfConstant('fMaxScale', tfFloat, 2)\n",
    "\n",
    "            tfTrainableTargetAverageUpdateCoef = tfConstant('fTrainableTargetAverageUpdateCoef', tfFloat, fTrainableTargetAverageUpdateCoef)\n",
    "            tfTrainableTargetAverageForgetCoef = tfConstant('fTrainableTargetAverageForgetCoef', tfFloat, 1.0 - fTrainableTargetAverageUpdateCoef)\n",
    "\n",
    "        with tf1.variable_scope('Var', reuse=tf1.AUTO_REUSE):\n",
    "            # Текущий эпизод\n",
    "            self.tuEpisode = tfGlobalVariable('uEpisode', tfInt, 1)\n",
    "            aTrainVariables.append(self.tuEpisode)\n",
    "\n",
    "        # Тело генетического буфера повтора\n",
    "\n",
    "        with tf.name_scope('ReplayBuffer'):\n",
    "            with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "                # Вместительность циклического буфера\n",
    "                self.tuReplayCapacity = tfConstant('uCapacity', tfInt, uReplayCapacity)\n",
    "                # Указатель начала в циклическом буфере\n",
    "                self.tuReplayStart = tfGlobalVariable('uStart', tfInt, 0)\n",
    "                aReplayBufferVariables.append(self.tuReplayStart)\n",
    "                # Указатель конца в циклическом буфере\n",
    "                self.tuReplayEnd = tfGlobalVariable('uEnd', tfInt, 0)\n",
    "                aReplayBufferVariables.append(self.tuReplayEnd)\n",
    "\n",
    "                # Настройки древовидного буфера\n",
    "                if self.bPriorityMode:\n",
    "                    uTreeSize = int(math.pow(2, math.ceil(math.log2(uReplayCapacity))))\n",
    "                    tuReplayTreeSize = tfConstant('uTreeSize', tfInt, uTreeSize)\n",
    "                    tuHalfReplayTreeSize = tfConstant('uHalfTreeSize', tfInt, uTreeSize>>1)\n",
    "                    tauOne = tfConstant('auOne', (tfInt, [1]), [1])\n",
    "\n",
    "                    # Коэффициенты для вычисления приоритета\n",
    "                    # `priority = clip(pow(error + eps, -power), min, max)`\n",
    "                    fErrorPower = 0.6\n",
    "                    fMinPriority = 0.1\n",
    "                    fMaxPriority = 1.0\n",
    "\n",
    "                    tfReplayErrorPower = tfConstant('fErrorPower', tfFloat, fErrorPower)\n",
    "                    tfReplayMinPriority = tfConstant('fMinPriority', tfFloat, fMinPriority)\n",
    "                    tfReplayMaxPriority = tfConstant('fMaxPriority', tfFloat, fMaxPriority)\n",
    "                    tafReplayMaxPriority = tfConstant('afMaxPriority', (tfFloat, [1]), [fMaxPriority])\n",
    "                    tfReplayPriorityEpsilon = tfConstant('fPriorityEpsilon', tfFloat, 0.01)\n",
    "\n",
    "                    # Коэффициенты для вычисления веса на основе приоритета\n",
    "                    fWeightPower = 0.4\n",
    "                    uWeightFeedSteps = 2e5\n",
    "\n",
    "                    tfReplayWeightDiff = tfConstant('fWeightDiff', tfFloat, (1.0 - fWeightPower) / float(uWeightFeedSteps))\n",
    "\n",
    "            with tf1.variable_scope('Var', reuse=tf1.AUTO_REUSE):\n",
    "                # Циклический буфер: наблюдения, действия, награды\n",
    "                self.tafReplayPrevObservations = tfGlobalVariable('afPrevObservations', (tfFloat, [uReplayCapacity, self.oPlayer.oEnvironment.uObservationSize]), 0)\n",
    "                aReplayBufferVariables.append(self.tafReplayPrevObservations)\n",
    "                self.tafReplayObservations = tfGlobalVariable('afObservations', (tfFloat, [uReplayCapacity, self.oPlayer.oEnvironment.uObservationSize]), 0)\n",
    "                aReplayBufferVariables.append(self.tafReplayObservations)\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    self.tauReplayActions = tfGlobalVariable('auActions', (tfInt, [uReplayCapacity]), 0)\n",
    "                    aReplayBufferVariables.append(self.tauReplayActions)\n",
    "                else:\n",
    "                    self.tafReplayActions = tfGlobalVariable('afActions', (tfFloat, [uReplayCapacity, self.oPlayer.oEnvironment.uActionsSize]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayActions)\n",
    "                if self.bEpisodeRating:\n",
    "                    self.tafReplayRatings = tfGlobalVariable('afRatings', (tfFloat, [uReplayCapacity]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayRatings)\n",
    "                else:\n",
    "                    self.tafReplayRewards = tfGlobalVariable('afRewards', (tfFloat, [uReplayCapacity]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayRewards)\n",
    "                self.tafReplayDones = tfGlobalVariable('afDones', (tfFloat, [uReplayCapacity]), 0)\n",
    "                aReplayBufferVariables.append(self.tafReplayDones)\n",
    "\n",
    "                # Древовидный буфер для быстрого поиска и случайной выборки с учётом приоритета\n",
    "                if self.bPriorityMode:\n",
    "                    # Степень значимости приоритетов (со временем стремится к 1)\n",
    "                    self.tfReplayWeightPower = tfGlobalVariable('fWeightPower', tfFloat, fWeightPower)\n",
    "                    aReplayBufferVariables.append(self.tfReplayWeightPower)\n",
    "\n",
    "                    # Древовидный буфер максимумов\n",
    "                    self.tafReplayMaxTree = tfGlobalVariable('afMaxTree', (tfFloat, [uTreeSize * 2]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayMaxTree)\n",
    "                    # Древовидный буфер сум\n",
    "                    self.tafReplaySumTree = tfGlobalVariable('afSumTree', (tfFloat, [uTreeSize * 2]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplaySumTree)\n",
    "\n",
    "            # Функциональный блок `fnAdd`, добавляющий записи в буфер повтора\n",
    "            with tf.name_scope('fnAdd'):\n",
    "                # Входные данные\n",
    "                with tf1.variable_scope('Input', reuse=tf1.AUTO_REUSE):\n",
    "                    self.tinafReplayPrevObservation = tfInput('afPrevObservation', (tfFloat, [self.oPlayer.oEnvironment.uObservationSize]))\n",
    "                    self.tinafReplayObservation = tfInput('afObservation', (tfFloat, [self.oPlayer.oEnvironment.uObservationSize]))\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        self.tinauReplayAction = tfInput('auAction', (tfInt, [1]))\n",
    "                    else:\n",
    "                        self.tinafReplayActions = tfInput('afActions', (tfFloat, [self.oPlayer.oEnvironment.uActionsSize]))\n",
    "                    if self.bEpisodeRating:\n",
    "                        self.tinafReplayRating = tfInput('afRating', (tfFloat, [1]))\n",
    "                    else:\n",
    "                        self.tinafReplayReward = tfInput('afReward', (tfFloat, [1]))\n",
    "                    self.tinfReplayDone = tfInput('fDone', tfFloat)\n",
    "                    self.tinfReplayScore = tfInput('fScore', tfFloat)\n",
    "                    self.tinuReplaySteps = tfInput('uSteps', tfInt)\n",
    "\n",
    "                tafDones = tf.expand_dims(self.tinfReplayDone, axis=-1)\n",
    "\n",
    "                # Индексный лимит\n",
    "                tuMaxIndex = self.tuReplayStart + self.tuReplayCapacity\n",
    "\n",
    "                # Сохранить входные данные в буфере\n",
    "                tuIndex = tf.math.floormod(self.tuReplayEnd, self.tuReplayCapacity)\n",
    "                tauIndices = tf.expand_dims(tf.expand_dims(tuIndex, axis=-1), axis=-1)\n",
    "                topUpdate1 = self.tafReplayPrevObservations.scatter_nd_update(tauIndices, self.tinafReplayPrevObservation[None, :])\n",
    "                topUpdate2 = self.tafReplayObservations.scatter_nd_update(tauIndices, self.tinafReplayObservation[None, :])\n",
    "                if oPlayer.oEnvironment.bDiscrete:\n",
    "                    topUpdate3 = self.tauReplayActions.scatter_nd_update(tauIndices, self.tinauReplayAction)\n",
    "                else:\n",
    "                    topUpdate3 = self.tafReplayActions.scatter_nd_update(tauIndices, self.tinafReplayActions[None, :])\n",
    "                if self.bEpisodeRating:\n",
    "                    topUpdate4 = self.tafReplayRatings.scatter_nd_update(tauIndices, self.tinafReplayRating)\n",
    "                else:\n",
    "                    topUpdate4 = self.tafReplayRewards.scatter_nd_update(tauIndices, self.tinafReplayReward)\n",
    "                topUpdate5 = self.tafReplayDones.scatter_nd_update(tauIndices, tafDones)\n",
    "\n",
    "                with tfWait([topUpdate1, topUpdate2, topUpdate3, topUpdate4, topUpdate5]):\n",
    "                    tuNewEnd = self.tuReplayEnd.assign_add(tuOne)\n",
    "\n",
    "                with tfWait([tuNewEnd]):\n",
    "                    # Проверить буфер на переполнение\n",
    "                    def fnOverflow():\n",
    "                        return self.tuReplayStart.assign(self.tuReplayEnd + tuOne - self.tuReplayCapacity)\n",
    "                    def fnNoOverflow():\n",
    "                        return self.tuReplayStart\n",
    "                    tuNewStart = tf.cond(tf.greater(self.tuReplayEnd, tuMaxIndex), fnOverflow, fnNoOverflow, 'uNewStart')\n",
    "\n",
    "                aWaitList = [tuNewStart]\n",
    "\n",
    "                # Логировать конечный счёт эпизода\n",
    "                if self.uLogLevel > 0:\n",
    "                    def fnAddLogEpisode():\n",
    "                        with self.oSummaryWriter.as_default(): # pylint: disable=not-context-manager\n",
    "                            topLogScore = tf.summary.scalar('Stats/Score', self.tinfReplayScore, tf.cast(self.tuEpisode, tf.int64))\n",
    "                            if self.uLogLevel > 1:\n",
    "                                topLogSteps = tf.summary.scalar('Info/Steps', self.tinuReplaySteps, tf.cast(self.tuEpisode, tf.int64))\n",
    "                                topLog = tf.group(topLogScore, topLogSteps)\n",
    "                            else:\n",
    "                                topLog = topLogScore\n",
    "                        return topLog\n",
    "                    def fnAddLogStep():\n",
    "                        return tf.no_op()\n",
    "                    aWaitList.append(tf.cond(tf.equal(self.tinfReplayDone, tfZero), true_fn=fnAddLogStep, false_fn=fnAddLogEpisode))\n",
    "\n",
    "                with tfWait(aWaitList):\n",
    "                    # Перейти к следующему эпизоду\n",
    "                    def fnNextEpisode():\n",
    "                        return self.tuEpisode.assign_add(tuOne)\n",
    "                    def fnCurEpisode():\n",
    "                        return self.tuEpisode\n",
    "                    tuNewEpisode = tf.cond(tf.not_equal(self.tinfReplayDone, tfZero), fnNextEpisode, fnCurEpisode, 'uNewEpisode')\n",
    "\n",
    "                # Обновить приоритеты в древовидном буфере\n",
    "                if self.bPriorityMode:\n",
    "                    tuIndex = tuIndex + tuReplayTreeSize\n",
    "                    tauIndices = tf.expand_dims(tf.expand_dims(tuIndex, axis=-1), axis=-1)\n",
    "                    topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauIndices, tafReplayMaxPriority)\n",
    "                    topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauIndices, tafReplayMaxPriority)\n",
    "                    tuIndex = tf.math.floordiv(tuIndex, tuTwo)\n",
    "                    with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                        def fnAddCompare(tuLoopIndex):\n",
    "                            return tf.greater_equal(tuLoopIndex, tuOne)\n",
    "                        def fnAddLoop(tuLoopIndex):\n",
    "                            tauLeft = tf.expand_dims(tuLoopIndex * tuTwo, axis=-1)\n",
    "                            tauRight = tauLeft + tuOne\n",
    "                            tfMax = tf.maximum(tf.gather(self.tafReplayMaxTree, tauLeft), tf.gather(self.tafReplayMaxTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tfSum = tf.add(tf.gather(self.tafReplaySumTree, tauLeft), tf.gather(self.tafReplaySumTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tauIndices = tf.expand_dims(tf.expand_dims(tuLoopIndex, axis=-1), axis=-1)\n",
    "                            topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauIndices, tfMax)\n",
    "                            topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauIndices, tfSum)\n",
    "                            with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                                tuLoopIndex = tf.math.floordiv(tuLoopIndex, tuTwo)\n",
    "                                return [tuLoopIndex]\n",
    "                        [topUpdateTree] = tf.while_loop(fnAddCompare, fnAddLoop, [tuIndex])\n",
    "\n",
    "            # Узел результата функции добавления данных в буфер повтора\n",
    "            if self.bPriorityMode:\n",
    "                self.tfnReplayAdd = tf.group(tuNewEpisode, topUpdateTree)\n",
    "            else:\n",
    "                self.tfnReplayAdd = tuNewEpisode\n",
    "\n",
    "            # Функциональный блок `fnReadBatch`, считывающий блок записей из буфера повтора\n",
    "            with tf.name_scope('fnReadBatch'):\n",
    "                # Создать блок индексов для выборки из буфера повтора\n",
    "                if self.bPriorityMode:\n",
    "                    # Общая сумма всех приоритетов\n",
    "                    tfTotalPriority = tf.squeeze(tf.gather(self.tafReplaySumTree, tauOne)) # pylint: disable=no-value-for-parameter\n",
    "                    # Набор случайных смещений приоритетов\n",
    "                    tafRandomPriorities = tf.random.uniform([uBatchSize], minval=tfZero, maxval=tfTotalPriority, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                    # Найти соответствующие смещениям приоритетов индексы\n",
    "                    tuIndex = tuOne\n",
    "                    tauIndices = tf.ones([uBatchSize], dtype=tfInt)\n",
    "                    def fnRandomCompare(tuLoopIndex, tauIndices, tafRandomPriorities):\n",
    "                        return tf.less(tuLoopIndex, tuReplayTreeSize)\n",
    "                    def fnRandomLoop(tuLoopIndex, tauIndices, tafRandomPriorities):\n",
    "                        tauLeft = tauIndices * tuTwo\n",
    "                        tafValues = tf.gather(self.tafReplaySumTree, tauLeft) # pylint: disable=no-value-for-parameter\n",
    "                        tabLessEqual = tf.less_equal(tafValues, tafRandomPriorities)\n",
    "                        return [tuLoopIndex * tuTwo, tauLeft + tf.cast(tabLessEqual, tfInt), tafRandomPriorities - tafValues * tf.cast(tabLessEqual, tfFloat)]\n",
    "                    [tuReturnIndex, tauReplayIndices, tafReturnRandomPriorities] = tf.while_loop(fnRandomCompare, fnRandomLoop, [tuIndex, tauIndices, tafRandomPriorities])\n",
    "                    # Расчитать весовые Коэффициенты для каждого индекса на основе его приоритета\n",
    "                    tfMaxPriorityScale = tfOne / tf.squeeze(tf.gather(self.tafReplayMaxTree, tauOne)) # pylint: disable=no-value-for-parameter\n",
    "                    tafBatchPriorities = tf.gather(self.tafReplaySumTree, tauReplayIndices) # pylint: disable=no-value-for-parameter\n",
    "                    tafBatchWeights = tf.pow(tafBatchPriorities * tfMaxPriorityScale, self.tfReplayWeightPower) # pylint: disable=invalid-unary-operand-type\n",
    "                    # Обновить коэффициент влияющий на веса\n",
    "                    topUpdateReplayWeightPower = self.tfReplayWeightPower.assign(tf.minimum(tfOne, self.tfReplayWeightPower + tfReplayWeightDiff))\n",
    "\n",
    "                    with tfWait([tuReturnIndex, tauReplayIndices, tafReturnRandomPriorities, topUpdateReplayWeightPower]):\n",
    "                        # Набор случайных индексов\n",
    "                        tauRandomIndices = tauReplayIndices - tuReplayTreeSize\n",
    "                else:\n",
    "                    # Набор случайных смещений\n",
    "                    tauRandomOffsets = tf.random.uniform([uBatchSize], minval=self.tuReplayStart, maxval=self.tuReplayEnd, dtype=tfInt, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                    # Набор случайных индексов\n",
    "                    tauRandomIndices = tf.math.floormod(tauRandomOffsets, self.tuReplayCapacity)\n",
    "\n",
    "                # Создать блок данных на основе выборки по случайным индексам\n",
    "                tafBatchPrevObservations = tf.gather(self.tafReplayPrevObservations, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                tafBatchObservations = tf.gather(self.tafReplayObservations, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tauBatchActions = tf.gather(self.tauReplayActions, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                else:\n",
    "                    tafBatchActions = tf.gather(self.tafReplayActions, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                if self.bEpisodeRating:\n",
    "                    tafBatchRatings = tf.gather(self.tafReplayRatings, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                else:\n",
    "                    tafBatchRewards = tf.gather(self.tafReplayRewards, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                tafBatchDones = tf.gather(self.tafReplayDones, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "\n",
    "        # Тело агента `Soft Actor Critic`\n",
    "\n",
    "        with tf.name_scope('SacAgent'):\n",
    "            with tf.name_scope('Critic'):\n",
    "                # Создать две нейросети `Критик-тренер` и две нейросети `Критик-цель`\n",
    "                self.annCriticNetworks = [CriticNetwork() for _ in range(4)]\n",
    "                self.nnTrainCritic1 = self.annCriticNetworks[0]\n",
    "                self.nnTrainCritic2 = self.annCriticNetworks[1]\n",
    "                self.nnTargetCritic1 = self.annCriticNetworks[2]\n",
    "                self.nnTargetCritic2 = self.annCriticNetworks[3]\n",
    "                [nnCritic.build((self.uBatchSize, oEnvironment.uObservationSize), (self.uBatchSize, oEnvironment.uActionsSize)) for nnCritic in self.annCriticNetworks]\n",
    "                [aTrainVariables.extend(nnCritic.trainable_variables) for nnCritic in self.annCriticNetworks]\n",
    "\n",
    "            with tf.name_scope('Actor'):\n",
    "                # Создать нейросеть `Актёр-тренер`\n",
    "                self.nnTrainActor = CreateActor(oEnvironment, self.uBatchSize, 'nnSacTrainActor')\n",
    "                aTrainVariables.extend(self.nnTrainActor.trainable_variables)\n",
    "\n",
    "                if not self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    afActionsMin = np.array([0])\n",
    "                    afActionsMax = np.array([1])\n",
    "\n",
    "                    # Константы непрерывной модели\n",
    "                    with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "                        tafActionsMean = tfConstant('afActionsMean', tfFloat, (afActionsMin + afActionsMax) / 2)\n",
    "                        tafActionsStd = tfConstant('afActionsStd', tfFloat, (afActionsMin - afActionsMax) / 2)\n",
    "\n",
    "            with tf1.variable_scope('Var', reuse=tf1.AUTO_REUSE):\n",
    "                # `Альфа-регулятор` \n",
    "                self.tfLogAlpha = tfGlobalVariable('fLogAlpha', tfFloat, 0, True)\n",
    "                aTrainVariables.append(self.tfLogAlpha)\n",
    "\n",
    "                if self.uLogLevel > 1:\n",
    "                    tfCriticGradientAverageNormal = tfGlobalVariable('fCriticGradientClip', tfFloat, 0)\n",
    "                    aTrainVariables.append(tfCriticGradientAverageNormal)\n",
    "                    tfActorGradientAverageNormal = tfGlobalVariable('fActorGradientClip', tfFloat, 0)\n",
    "                    aTrainVariables.append(tfActorGradientAverageNormal)\n",
    "\n",
    "                if tfTrainStepCounter is None:\n",
    "                    tfTrainStepCounter = tf.compat.v1.train.get_or_create_global_step()\n",
    "                aTrainVariables.append(tfTrainStepCounter)\n",
    "\n",
    "            with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "                # Фактор забывания оценки\n",
    "                tfRatingDiscountFactor = tfConstant('fRatingDiscountFactor', tfFloat, self.fRatingDiscountFactor)\n",
    "                # Желаемая энтропия\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tfTargetEntropy = tfConstant('fTargetEntropy', tfFloat, -np.log(1.0 / oEnvironment.uActionsSize) * 0.98)\n",
    "                else:\n",
    "                    tfTargetEntropy = tfConstant('fTargetEntropy', tfFloat, oEnvironment.uActionsSize / 2.0)\n",
    "\n",
    "            # Функциональный блок `fnInitialize`, производящий инициализацию всех переменных при первом запуске\n",
    "            with tf.name_scope('fnInitialize'):\n",
    "                topCriticUpdate1 = fnHardUpdate(self.nnTargetCritic1.variables, self.nnTrainCritic1.variables)\n",
    "                topCriticUpdate2 = fnHardUpdate(self.nnTargetCritic2.variables, self.nnTrainCritic2.variables)\n",
    "                topActorUpdate = fnHardUpdate(self.oPlayer.nnActor.variables, self.nnTrainActor.variables)\n",
    "            # Узел результата функции инициализации\n",
    "            self.tfnInitialize = tf.group(topCriticUpdate1, topCriticUpdate2, topActorUpdate)\n",
    "\n",
    "            # Обновить целевые нейронные сети\n",
    "            def fnUpdateTarget():\n",
    "                topCriticUpdate1 = fnSoftUpdate(self.nnTargetCritic1.variables, self.nnTrainCritic1.variables, tfZero, tfOne, tfTrainableTargetAverageForgetCoef, tfTrainableTargetAverageUpdateCoef)\n",
    "                topCriticUpdate2 = fnSoftUpdate(self.nnTargetCritic2.variables, self.nnTrainCritic2.variables, tfZero, tfOne, tfTrainableTargetAverageForgetCoef, tfTrainableTargetAverageUpdateCoef)\n",
    "                topActorUpdate = fnHardUpdate(self.oPlayer.nnActor.variables, self.nnTrainActor.variables)\n",
    "                return tf.group(topCriticUpdate1, topCriticUpdate2, topActorUpdate)\n",
    "\n",
    "            # Функциональный блок `fnTrain`, выполняющий все операции, связанные с обучением сетей\n",
    "            with tf.name_scope('fnTrain'):\n",
    "                # Форматировать входные данные\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tafActionsOneHots = tf.one_hot(tauBatchActions, tuActionsSize, on_value=tfOne, off_value=tfZero, dtype=tfFloat) # pylint: disable=unexpected-keyword-arg\n",
    "                    tafStates = self.nnTrainCritic1.prepare(tafBatchPrevObservations, tafActionsOneHots)\n",
    "                else:\n",
    "                    tafStates = self.nnTrainCritic1.prepare(tafBatchPrevObservations, tafBatchActions)\n",
    "\n",
    "                # Рассчитать рейтинги для входных данных\n",
    "                if self.bEpisodeRating:\n",
    "                    # Для эпизодического режима, рейтинги уже рассчитаны заранее\n",
    "                    tafRatings = tafBatchRatings\n",
    "                else:\n",
    "                    # Рассчитать вероятности следующих действий\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        # Ненормализованные предполагаемые логарифмические вероятности возможных действий\n",
    "                        tafNextUnnormalizedLogProbabilities = self.nnTrainActor.call(tafBatchObservations)\n",
    "                        # Массив случайных чисел для добавления шума к логарифмическим вероятностям\n",
    "                        tafRandomUniforms = tf.random.uniform([self.uBatchSize, oEnvironment.uActionsSize], minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                        # Распределение Гумбеля для случайных чисел\n",
    "                        tafGumbels = -tf.math.log(-tf.math.log(tafRandomUniforms)) # pylint: disable=invalid-unary-operand-type\n",
    "                        # Ненормализованные предполагаемые логарифмические вероятности возможных действий с добавлением шума\n",
    "                        tafNextUnnormalizedNoisyLogProbabilities = tafNextUnnormalizedLogProbabilities + tafGumbels\n",
    "                        # Наилучшие предполагаемые действия\n",
    "                        tauNextBestActions = tf.argmax(tafNextUnnormalizedNoisyLogProbabilities, axis=-1, output_type=tfInt)\n",
    "                        # Вектора наилучших предполагаемых действий\n",
    "                        tafNextPredActions = tf.one_hot(tauNextBestActions, tuActionsSize, on_value=tfOne, off_value=tfZero, dtype=tfFloat) # pylint: disable=unexpected-keyword-arg\n",
    "                        # Нормализованные предполагаемые логарифмические вероятности возможных действий\n",
    "                        tafNextNormalizedLogProbabilities = tf.math.log_softmax(tafNextUnnormalizedLogProbabilities, axis=-1)\n",
    "                        # Логарифмические вероятности наилучших предполагаемых действий (в роли энтропии)\n",
    "                        tafNextProbabilitiesEntropy = -tf.math.reduce_sum(tafNextPredActions * tafNextNormalizedLogProbabilities, axis=-1)\n",
    "                    else:\n",
    "                        tafNextLocations, tafNextScales = self.nnTrainActor.call(tafBatchObservations)\n",
    "                        tafNextClippedScales = tf.clip_by_value(tafNextScales, tfTotalMinScale, tfTotalMaxScale)\n",
    "                        tafNextClippedScalesExp = tf.math.exp(tafNextClippedScales)\t\t\t\n",
    "                        tafNextRandomNormals = tf.random.normal([self.uBatchSize, oEnvironment.uActionsSize], mean=tafNextLocations, stddev=tafNextClippedScalesExp, dtype=tfFloat, seed=None)\n",
    "                        tafNextPredActions = tf.math.tanh(tafNextRandomNormals) * tafActionsStd + tafActionsMean\n",
    "                        tafNextClippedScalesExpSquare = tf.square(tafNextClippedScalesExp)\n",
    "                        tafNextProbabilitiesEntropy = (tf.square(tafNextPredActions - tafNextLocations)) / (2 * tafNextClippedScalesExpSquare) + tafNextClippedScales + tfLogSqrtPi2 # pylint: disable=invalid-unary-operand-type\n",
    "\n",
    "                    # Рассчитать следующие вероятные рейтинги\n",
    "                    tafNextStates = self.nnTargetCritic1.prepare(tafBatchObservations, tafNextPredActions)\n",
    "                    tafPredNextRatings1 = tf.squeeze(self.nnTargetCritic1.call(tafNextStates), axis=-1)\n",
    "                    tafPredNextRatings2 = tf.squeeze(self.nnTargetCritic2.call(tafNextStates), axis=-1)\n",
    "                    tafPredNextRatings = tf.minimum(tafPredNextRatings1, tafPredNextRatings2) + tf.math.exp(self.tfLogAlpha) * tafNextProbabilitiesEntropy\n",
    "\n",
    "                    # Рассчитать рейтинги\n",
    "                    tafRatings = tafBatchRewards + (1. - tafBatchDones) * tafPredNextRatings * tfRatingDiscountFactor\n",
    "\n",
    "                # Вычислить ошибку обучения нейросети `Критик-тренер` #1\n",
    "                tfaTrainableCriticVariables1 = self.nnTrainCritic1.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableCriticVariables1)\n",
    "\n",
    "                    # Предполагаемые рейтинги\n",
    "                    tafPredRatings1 = tf.squeeze(self.nnTrainCritic1.call(tf.stop_gradient(tafStates)), axis=-1)\n",
    "                    # Приимущество реальых рейтингов перед предугаданными\n",
    "                    tafAdvantages1 = tf.stop_gradient(tafRatings) - tafPredRatings1\n",
    "                    # Ошибка обучения\n",
    "                    if self.bPriorityMode:\n",
    "                        tfCriticLoss1 = tf.reduce_mean(tf.square(tafAdvantages1) * tf.stop_gradient(tafBatchWeights))\n",
    "                    else:\n",
    "                        tfCriticLoss1 = tf.reduce_mean(tf.square(tafAdvantages1))\n",
    "\n",
    "                # Вычислить градиенты\n",
    "                aCriticGradients1 = tape.gradient(tfCriticLoss1, tfaTrainableCriticVariables1)\n",
    "\n",
    "                # Вычислить ошибку обучения нейросети `Критик-тренер` #2\n",
    "                tfaTrainableCriticVariables2 = self.nnTrainCritic2.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableCriticVariables2)\n",
    "\n",
    "                    # Предполагаемые рейтинги\n",
    "                    tafPredRatings2 = tf.squeeze(self.nnTrainCritic2.call(tf.stop_gradient(tafStates)), axis=-1)\n",
    "                    # Приимущество реальых рейтингов перед предугаданными\n",
    "                    tafAdvantages2 = tf.stop_gradient(tafRatings) - tafPredRatings2\n",
    "                    # Ошибка обучения\n",
    "                    if self.bPriorityMode:\n",
    "                        tfCriticLoss2 = tf.reduce_mean(tf.square(tafAdvantages2) * tf.stop_gradient(tafBatchWeights))\n",
    "                    else:\n",
    "                        tfCriticLoss2 = tf.reduce_mean(tf.square(tafAdvantages2))\n",
    "\n",
    "                # Вычислить градиенты\n",
    "                aCriticGradients2 = tape.gradient(tfCriticLoss2, tfaTrainableCriticVariables2)\n",
    "\n",
    "                # Общая ошибка критика\n",
    "                tfCriticLoss = (tfCriticLoss1 + tfCriticLoss2) * tfHalf\n",
    "\n",
    "                # Рассчитать усреднённое нормальное значение градентов `Критиков-тренеров`\n",
    "                if self.uLogLevel > 1:\n",
    "                    tuGradientsCount = tfZero\n",
    "                    tfGradientsNormalMass = tfZero\n",
    "                    for tfaGradientsGroup in aCriticGradients1:\n",
    "                        tfaGradients = tfaGradientsGroup.values if isinstance(tfaGradientsGroup, tf.IndexedSlices) else tfaGradientsGroup\n",
    "                        tuCount = tf.cast(tf.size(tfaGradients), dtype=tfFloat) # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n",
    "                        tuGradientsCount += tuCount\n",
    "                        tfGradientsNormalMass += tf.linalg.global_norm([tfaGradients]) * tuCount\n",
    "                    for tfaGradientsGroup in aCriticGradients2:\n",
    "                        tfaGradients = tfaGradientsGroup.values if isinstance(tfaGradientsGroup, tf.IndexedSlices) else tfaGradientsGroup\n",
    "                        tuCount = tf.cast(tf.size(tfaGradients), dtype=tfFloat) # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n",
    "                        tuGradientsCount += tuCount\n",
    "                        tfGradientsNormalMass += tf.linalg.global_norm([tfaGradients]) * tuCount\n",
    "                    tfCriticGradientNormal = tfCriticGradientAverageNormal.assign(tfCriticGradientAverageNormal * tfGradientNormalForgetCoef + (tfGradientsNormalMass / tuGradientsCount) * tfGradientNormalUpdateCoef)\n",
    "\n",
    "                # Обрезать значения градиентов для предотвращения ошибок `inf` и `nan`\n",
    "                if fMaxGradientNormal is not None:\n",
    "                    aCriticGradients1 = fnClipGradients(aCriticGradients1, tfMaxGradientNormal)\n",
    "                    aCriticGradients2 = fnClipGradients(aCriticGradients2, tfMaxGradientNormal)\n",
    "\n",
    "                # Обновить приоритеты в буфере повтора\n",
    "                if self.bPriorityMode:\n",
    "                    # Вычислить приоритеты на основе приимущества рейтингов\n",
    "                    tafReplayErrors = tf.minimum(tf.abs(tafAdvantages1), tf.abs(tafAdvantages2))\n",
    "                    tafReplayPriorities = tf.clip_by_value(tf.pow(tafReplayErrors + tfReplayPriorityEpsilon, -tfReplayErrorPower), tfReplayMinPriority, tfReplayMaxPriority)\n",
    "                    # Обновить приоритеты\n",
    "                    tauUpdateIndices = tf.expand_dims(tauReplayIndices, axis=-1)\n",
    "                    topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauUpdateIndices, tafReplayPriorities)\n",
    "                    topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauUpdateIndices, tafReplayPriorities)\n",
    "                    with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                        tuIndex = tuHalfReplayTreeSize\n",
    "                        tauIndices = tf.math.floordiv(tauReplayIndices, tuTwo)\n",
    "                        def fnUpdateCompare(tuLoopIndex, tauIndices):\n",
    "                            return tf.greater_equal(tuLoopIndex, tuOne)\n",
    "                        def fnUpdateLoop(tuLoopIndex, tauIndices):\n",
    "                            tauLeft = tauIndices * tuTwo\n",
    "                            tauRight = tauLeft + tuOne\n",
    "                            tfMax = tf.maximum(tf.gather(self.tafReplayMaxTree, tauLeft), tf.gather(self.tafReplayMaxTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tfSum = tf.add(tf.gather(self.tafReplaySumTree, tauLeft), tf.gather(self.tafReplaySumTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tauUpdateIndices = tf.expand_dims(tauIndices, axis=-1)\n",
    "                            topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauUpdateIndices, tfMax)\n",
    "                            topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauUpdateIndices, tfSum)\n",
    "                            with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                                tauIndices = tf.math.floordiv(tauIndices, tuTwo)\n",
    "                                tuLoopIndex = tf.math.floordiv(tuLoopIndex, tuTwo)\n",
    "                                return [tuLoopIndex, tauIndices]\n",
    "                        [tuIndex, tauIndices] = tf.while_loop(fnUpdateCompare, fnUpdateLoop, [tuIndex, tauIndices])\n",
    "                        topUpdateReplayPriorities = tf.group(tuIndex, tauIndices)\n",
    "\n",
    "                # Вычислить ошибку обучения нейросети `Актёр-тренер`\n",
    "                tfaTrainableActorVariables = self.nnTrainActor.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableActorVariables)\n",
    "\n",
    "                    # Рассчитать вероятности всех возможных действий\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        # Ненормализованные предполагаемые логарифмические вероятности действий\n",
    "                        tafUnnormalizedLogProbabilities = self.nnTrainActor.call(tf.stop_gradient(tafBatchPrevObservations))\n",
    "                        # Массив случайных чисел для добавления шума к логарифмическим вероятностям\n",
    "                        tafRandomUniforms = tf.random.uniform([self.uBatchSize, oEnvironment.uActionsSize], minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                        # Распределение Гумбеля для случайных чисел\n",
    "                        tafGumbels = -tf.math.log(-tf.math.log(tafRandomUniforms)) # pylint: disable=invalid-unary-operand-type\n",
    "                        # Ненормализованные предполагаемые логарифмические вероятности возможных действий с добавлением шума\n",
    "                        tafUnnormalizedNoisyLogProbabilities = tafUnnormalizedLogProbabilities + tafGumbels\n",
    "                        # Вектора предполагаемых действий\n",
    "                        tafPredActions = tf.math.exp(tafUnnormalizedNoisyLogProbabilities - tf.math.reduce_logsumexp(tafUnnormalizedNoisyLogProbabilities, axis=-1, keepdims=True))\n",
    "                        # Нормализованные предполагаемые логарифмические вероятности возможных действий\n",
    "                        tafPredNormalizedLogProbabilities = tf.math.log_softmax(tafUnnormalizedLogProbabilities, axis=-1)\n",
    "                        # Логарифмические вероятности наилучших предполагаемых действий (в роли энтропии)\n",
    "                        tafPredProbabilitiesEntropy = -tf.math.reduce_sum(tafPredActions * tafPredNormalizedLogProbabilities, axis=-1)\n",
    "                    else:\n",
    "                        tafLocations, tafScales = self.nnTrainActor.call(tf.stop_gradient(tafBatchPrevObservations))\n",
    "                        tafClippedScales = tf.clip_by_value(tafScales, tfTotalMinScale, tfTotalMaxScale)\n",
    "                        tafClippedScalesExp = tf.math.exp(tafClippedScales)\n",
    "                        tafRandomNormals = tf.random.normal([self.uBatchSize,oEnvironment.uActionsSize], mean=tfZero, stddev=tfOne, dtype=tfFloat, seed=None)\n",
    "                        tafRandomUnscaledActions = tafLocations + tafClippedScalesExp * tafRandomNormals\n",
    "                        tafPredActions = tf.math.tanh(tafRandomUnscaledActions) * tafActionsStd + tafActionsMean\n",
    "                        tafClippedScalesExpPower = tf.math.square(tafClippedScalesExp)\n",
    "                        tafNormalizers = -tf.math.reduce_sum(tf.math.log(1 - tf.math.square(tafPredActions) + 1e-6), axis=1)\n",
    "                        tafPredProbabilitiesEntropy = (tf.math.square(tafRandomUnscaledActions - tafLocations)) / (2 * tafClippedScalesExpPower) + tafClippedScales + tfLogSqrtPi2 + tafNormalizers # pylint: disable=invalid-unary-operand-type\n",
    "\n",
    "                    # Рассчитать предполагаемые вероятные рейтинги\n",
    "                    tafRandomStates = self.nnTrainCritic1.prepare(tf.stop_gradient(tafBatchPrevObservations), tafPredActions)\n",
    "                    tafRandomPredRatings1 = tf.squeeze(self.nnTrainCritic1.call(tafRandomStates), axis=-1)\n",
    "                    tafRandomPredRatings2 = tf.squeeze(self.nnTrainCritic2.call(tafRandomStates), axis=-1)\n",
    "                    tafRandomPredRatings = tf.minimum(tafRandomPredRatings1, tafRandomPredRatings2) + tf.exp(self.tfLogAlpha) * tafPredProbabilitiesEntropy\n",
    "\n",
    "                    # Ошибка обучения\n",
    "                    if self.bPriorityMode:\n",
    "                        tfActorLoss = -tf.math.reduce_mean(tafRandomPredRatings * tf.stop_gradient(tafBatchWeights))\n",
    "                    else:\n",
    "                        tfActorLoss = -tf.math.reduce_mean(tafRandomPredRatings)\n",
    "\n",
    "                # Вычислить градиенты\n",
    "                aActorGradients = tape.gradient(tfActorLoss, tfaTrainableActorVariables)\n",
    "\n",
    "                # Рассчитать усреднённое нормальное значение градентов `Актёра-тренера`\n",
    "                if self.uLogLevel > 1:\n",
    "                    tuGradientsCount = tfZero\n",
    "                    tfGradientsNormalMass = tfZero\n",
    "                    for tfaGradientsGroup in aActorGradients:\n",
    "                        tfaGradients = tfaGradientsGroup.values if isinstance(tfaGradientsGroup, tf.IndexedSlices) else tfaGradientsGroup\n",
    "                        tuCount = tf.cast(tf.size(tfaGradients), dtype=tfFloat) # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n",
    "                        tuGradientsCount += tuCount\n",
    "                        tfGradientsNormalMass += tf.linalg.global_norm([tfaGradients]) * tuCount\n",
    "                    tfActorGradientNormal = tfActorGradientAverageNormal.assign(tfActorGradientAverageNormal * tfGradientNormalForgetCoef + (tfGradientsNormalMass / tuGradientsCount) * tfGradientNormalUpdateCoef)\n",
    "\n",
    "                # Обрезать значения градиентов для предотвращения ошибок `inf` и `nan`\n",
    "                if fMaxGradientNormal is not None:\n",
    "                    aActorGradients = fnClipGradients(aActorGradients, tfMaxGradientNormal)\n",
    "\n",
    "                # Вычислить ошибку обучения `Альфа-регулятора`\n",
    "                tfaTrainableAlphaVariable = [self.tfLogAlpha]\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableAlphaVariable)\n",
    "\n",
    "                    # Вычислить ошибку энтропии\n",
    "                    tafEntropyLoss = -(tafPredProbabilitiesEntropy - tfTargetEntropy)\n",
    "\n",
    "                    # Ошибка обучения\n",
    "                    if self.bPriorityMode:\n",
    "                        tfAlphaLoss = tf.math.reduce_mean(tafEntropyLoss * tf.stop_gradient(tafBatchWeights)) * self.tfLogAlpha\n",
    "                    else:\n",
    "                        tfAlphaLoss = tf.math.reduce_mean(tafEntropyLoss) * self.tfLogAlpha\n",
    "\n",
    "                # Вычислить градиенты\n",
    "                aAlphaGradient = tape.gradient(tfAlphaLoss, tfaTrainableAlphaVariable)\n",
    "\n",
    "                # Обрезать значения градиентов для предотвращения ошибок `inf` и `nan`\n",
    "                if fMaxGradientNormal is not None:\n",
    "                    aAlphaGradient = fnClipGradients(aAlphaGradient, tfMaxGradientNormal)\n",
    "\n",
    "                # Обучение\n",
    "                with tfWait([tfCriticLoss, tfActorLoss, tfAlphaLoss]):\n",
    "                    topOptimizeCritic1 = self.koCriticOptimizer.apply_gradients(zip(aCriticGradients1, tfaTrainableCriticVariables1))\n",
    "                    topOptimizeCritic2 = self.koCriticOptimizer.apply_gradients(zip(aCriticGradients2, tfaTrainableCriticVariables2))\n",
    "                    topOptimizeActor = self.koActorOptimizer.apply_gradients(zip(aActorGradients, tfaTrainableActorVariables))\n",
    "                    topOptimizeAlpha = self.koAlphaOptimizer.apply_gradients(zip(aAlphaGradient, tfaTrainableAlphaVariable))\n",
    "\n",
    "                aWaitList = [topOptimizeCritic1, topOptimizeCritic2, topOptimizeActor, topOptimizeAlpha]\n",
    "\n",
    "                if self.bPriorityMode:\n",
    "                    aWaitList.append(topUpdateReplayPriorities)\n",
    "\n",
    "        # Функциональный субблок логирования, выполняющий все операции, связанные со сбором статистики\n",
    "\n",
    "        if self.uLogLevel > 0:\n",
    "            with self.oSummaryWriter.as_default(): # pylint: disable=not-context-manager\n",
    "                aWaitList.append(tf.summary.scalar('Stats/Loss/Critic', tf.reduce_mean(tfCriticLoss), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Stats/Loss/Actor', tf.reduce_mean(tfActorLoss), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Stats/Loss/Alpha', tfAlphaLoss, tfTrainStepCounter))\n",
    "\n",
    "        if self.uLogLevel > 1:\n",
    "            with self.oSummaryWriter.as_default(): # pylint: disable=not-context-manager\n",
    "                aWaitList.append(tf.summary.scalar('Info/GradientNormal/Critic', tfCriticGradientNormal, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Info/GradientNormal/Actor', tfActorGradientNormal, tfTrainStepCounter))\n",
    "                if self.bPriorityMode:\n",
    "                    aWaitList.append(tf.summary.scalar('Info/LossWeight/Mean', tf.reduce_mean(tafBatchWeights), tfTrainStepCounter))\n",
    "\n",
    "                aWaitList.append(tf.summary.scalar('Rating/LogAlpha', self.tfLogAlpha, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Rating/Alpha', tf.math.exp(self.tfLogAlpha), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Rating/Value/Mean', tf.reduce_mean(tafRatings), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Rating/Entropy/Mean', tf.reduce_mean(tafPredProbabilitiesEntropy), tfTrainStepCounter))\n",
    "\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Value', tafRatings, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Entropy', tafPredProbabilitiesEntropy, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Pred', tafRandomPredRatings, tfTrainStepCounter))\n",
    "                tafAdvantages = tf.concat([tafAdvantages1, tafAdvantages2], 0)\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Advantage', tafAdvantages, tfTrainStepCounter))\n",
    "\n",
    "        if self.uLogLevel > 2:\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aCriticGradients1, tfaTrainableCriticVariables1), tfTrainStepCounter))\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aCriticGradients2, tfaTrainableCriticVariables2), tfTrainStepCounter))\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aActorGradients, tfaTrainableActorVariables), tfTrainStepCounter))\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aAlphaGradient, tfaTrainableAlphaVariable), tfTrainStepCounter))\n",
    "\n",
    "        with tf.name_scope('SacAgent'):\n",
    "            # Продолжение... Функциональный блок `fnTrain`, выполняющий все операции, связанные с обучением сетей\n",
    "            with tf.name_scope('fnTrain'):\n",
    "                with tfWait(aWaitList):\n",
    "                    # Переход к следующему шагу\n",
    "                    topNewStepCounter = tfTrainStepCounter.assign_add(tuOne64)\n",
    "                    # Обновить целевые нейросети используя метод `скользящего окна`\n",
    "                    topUpdateTarget = fnUpdateTarget()\n",
    "\n",
    "                with tfWait([topNewStepCounter, topUpdateTarget]):\n",
    "                    # Узел результата функции тренировки\n",
    "                    self.tfnTrain = tfCriticLoss + tfActorLoss + tfAlphaLoss\n",
    "\n",
    "        # Сформировать папки для хранения модели\n",
    "\n",
    "        self.sTargetRestorePath = sRestorePath + 'Target/'\n",
    "        self.sTrainRestorePath = sRestorePath + 'Train/'\n",
    "        self.sReplayBufferRestorePath = sRestorePath + 'ReplayBuffer/'\n",
    "\n",
    "        self.oTargetSaver = tf.compat.v1.train.Saver(aTargetVariables, save_relative_paths=True)\n",
    "        self.oTrainSaver = tf.compat.v1.train.Saver(aTrainVariables, save_relative_paths=True)\n",
    "        self.oReplayBufferSaver = tf.compat.v1.train.Saver(aReplayBufferVariables, save_relative_paths=True)\n",
    "\n",
    "        if not os.path.exists(self.sTargetRestorePath):\n",
    "            os.makedirs(self.sTargetRestorePath, exist_ok=True)\n",
    "        if not os.path.exists(self.sTrainRestorePath):\n",
    "            os.makedirs(self.sTrainRestorePath, exist_ok=True)\n",
    "        if not os.path.exists(self.sReplayBufferRestorePath):\n",
    "            os.makedirs(self.sReplayBufferRestorePath, exist_ok=True)\n",
    "\n",
    "    # Внутренняя функция для заполнения буфера одним шагом\n",
    "    def __fill(self, uDebugLevel):\n",
    "        uEpisode = tfEval(self.tuEpisode)\n",
    "        bDone = self.oPlayer.next()\n",
    "        uFillCount = 0\n",
    "\n",
    "        if self.bEpisodeRating:\n",
    "            if self.uBufferSize >= self.uBufferCapacity:\n",
    "                self.uBufferCapacity += 256\n",
    "\n",
    "                self.afPrevObservations = np.resize(self.afPrevObservations, (self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize))\n",
    "                self.afObservations = np.resize(self.afObservations, (self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize))\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    self.auActions = np.resize(self.auActions, (self.uBufferCapacity,))\n",
    "                else:\n",
    "                    self.afActions= np.resize(self.afActions, (self.uBufferCapacity, self.oPlayer.oEnvironment.uActionsSize))\n",
    "                self.afRewards = np.resize(self.afRewards, (self.uBufferCapacity,))\n",
    "                self.afRatings = np.resize(self.afRatings, (self.uBufferCapacity,))\n",
    "\n",
    "            uIndex = self.uBufferSize\n",
    "\n",
    "            self.afPrevObservations[uIndex] = self.oPlayer.afPrevObservation\n",
    "            self.afObservations[uIndex] = self.oPlayer.afObservation\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                self.auActions[uIndex] = self.oPlayer.uAction\n",
    "            else:\n",
    "                self.afActions[uIndex] = self.oPlayer.afActions\n",
    "            self.afRewards[uIndex] = self.oPlayer.fReward\n",
    "\n",
    "            self.uBufferSize += 1\n",
    "        else:\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                tfEval(self.tfnReplayAdd, {\n",
    "                    self.tinafReplayPrevObservation: self.oPlayer.afPrevObservation,\n",
    "                    self.tinafReplayObservation: self.oPlayer.afObservation,\n",
    "                    self.tinauReplayAction: [self.oPlayer.uAction],\n",
    "                    self.tinafReplayReward: [self.oPlayer.fReward],\n",
    "                    self.tinfReplayDone: float(bDone),\n",
    "                    self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                    self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                })\n",
    "            else:\n",
    "                tfEval(self.tfnReplayAdd, {\n",
    "                    self.tinafReplayPrevObservation: self.oPlayer.afPrevObservation,\n",
    "                    self.tinafReplayObservation: self.oPlayer.afObservation,\n",
    "                    self.tinafReplayActions: self.oPlayer.afActions,\n",
    "                    self.tinafReplayReward: [self.oPlayer.fReward],\n",
    "                    self.tinfReplayDone: float(bDone),\n",
    "                    self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                    self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                })\n",
    "            uFillCount += 1\n",
    "\n",
    "        if bDone:\n",
    "            if self.bEpisodeRating:\n",
    "                uIndex = self.uBufferSize - 1\n",
    "                fLastScore = self.afRatings[uIndex] = self.afRewards[uIndex]\n",
    "                while uIndex > 0:\n",
    "                    uIndex -= 1\n",
    "                    fLastScore = self.afRatings[uIndex] = self.afRewards[uIndex] + self.fRatingDiscountFactor * fLastScore\n",
    "\n",
    "                for uIndex in range(self.uBufferSize - 1):\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        tfEval(self.tfnReplayAdd, {\n",
    "                            self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                            self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                            self.tinauReplayAction: [self.auActions[uIndex]],\n",
    "                            self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                            self.tinfReplayDone: 0.0,\n",
    "                            self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                            self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                        })\n",
    "                    else:\n",
    "                        tfEval(self.tfnReplayAdd, {\n",
    "                            self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                            self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                            self.tinafReplayActions: [self.afActions[uIndex]],\n",
    "                            self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                            self.tinfReplayDone: 0.0,\n",
    "                            self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                            self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                        })\n",
    "                    uFillCount += 1\n",
    "\n",
    "                uIndex = self.uBufferSize - 1\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tfEval(self.tfnReplayAdd, {\n",
    "                        self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                        self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                        self.tinauReplayAction: [self.auActions[uIndex]],\n",
    "                        self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                        self.tinfReplayDone: 1.0,\n",
    "                        self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                        self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                    })\n",
    "                else:\n",
    "                    tfEval(self.tfnReplayAdd, {\n",
    "                        self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                        self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                        self.tinafReplayActions: [self.afActions[uIndex]],\n",
    "                        self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                        self.tinfReplayDone: 1.0,\n",
    "                        self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                        self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                    })\n",
    "                uFillCount += 1\n",
    "\n",
    "                self.uBufferSize = 0\n",
    "\n",
    "        if uDebugLevel > 0:\n",
    "            uEnd, uStart, uReplayCapacity = tfEval([self.tuReplayEnd, self.tuReplayStart, self.tuReplayCapacity])\n",
    "\n",
    "            print('\\r[MEM:%s] Fill: Ep.%d:%d, Score %f, RewardAvg %f, Used %d of %d        ' % (\n",
    "                getMemoryUsage(),\n",
    "                uEpisode, self.oPlayer.uStep, self.oPlayer.fScore, self.oPlayer.fAverageReward,\n",
    "                uEnd - uStart,\n",
    "                uReplayCapacity\n",
    "            ), end = '\\n' if bDone else '')\n",
    "\n",
    "        if bDone:\n",
    "            self.oPlayer.reset()\n",
    "\n",
    "        return bDone, uFillCount\n",
    "\n",
    "    # Заполнить буфер повтора N шагами или N эпизодами, в зависимости от настройки\n",
    "    def fill(self, uFillSize=1, uDebugLevel=0, bPrefill=False):\n",
    "        if uDebugLevel > 1:\n",
    "            print('\\rFill: Processing...', end='')\n",
    "\n",
    "        uTotalFillCount = 0\n",
    "\n",
    "        for _ in range(uFillSize):\n",
    "            bDone, uFillCount = self.__fill(uDebugLevel)\n",
    "            uTotalFillCount += uFillCount\n",
    "\n",
    "        if bPrefill:\n",
    "            while not bDone:\n",
    "                bDone, uFillCount = self.__fill(uDebugLevel)\n",
    "                uTotalFillCount += uFillCount\n",
    "\n",
    "        if uDebugLevel > 1:\n",
    "            uEnd, uStart, uReplayCapacity = tfEval([self.tuReplayEnd, self.tuReplayStart, self.tuReplayCapacity])\n",
    "\n",
    "            print('\\nStatus: Used %d of %d from %d to %d' % (\n",
    "                uEnd - uStart,\n",
    "                uReplayCapacity,\n",
    "                uStart % uReplayCapacity,\n",
    "                uEnd % uReplayCapacity\n",
    "            ))\n",
    "\n",
    "        return uTotalFillCount, bDone\n",
    "\n",
    "    # Выполнить инициализацию графа\n",
    "    def initialize(self):\n",
    "        return tfEval([self.oSummaryWriter.init(), self.tfnInitialize])\n",
    "\n",
    "    # Сохранить модель\n",
    "    def save(self):\n",
    "        self.oTargetSaver.save(tfActiveSession.tfoSession, self.sTargetRestorePath)\n",
    "        self.oTrainSaver.save(tfActiveSession.tfoSession, self.sTrainRestorePath)\n",
    "        self.oReplayBufferSaver.save(tfActiveSession.tfoSession, self.sReplayBufferRestorePath)\n",
    "\n",
    "    # Восстановить модель\n",
    "    def restore(self):\n",
    "        try:\n",
    "            self.oTargetSaver.restore(tfActiveSession.tfoSession, self.sTargetRestorePath)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.oTrainSaver.restore(tfActiveSession.tfoSession, self.sTrainRestorePath)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.oReplayBufferSaver.restore(tfActiveSession.tfoSession, self.sReplayBufferRestorePath)\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    # Произвести один цикл тренировки\n",
    "    def train(self):\n",
    "        fStartTime = time.time()\n",
    "        fTotalLoss = tfEval(self.tfnTrain)\n",
    "        fTimeElapsed = time.time() - fStartTime\n",
    "\n",
    "        return fTotalLoss, fTimeElapsed\n",
    "\n",
    "    # Опустошить все буферы записи перед закрытием\n",
    "    def flush(self):\n",
    "        tfEval(self.oSummaryWriter.flush())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировочный цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:46:32.827452Z",
     "start_time": "2020-06-29T17:51:53.557429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEM:507957248:3941535744] Fill: Ep.1:65, Score -101.495070, RewardAvg -0.101479, Used 65 of 131072        \n",
      "[MEM:508227584:3941535744] Fill: Ep.2:111, Score -466.828336, RewardAvg -0.451310, Used 176 of 131072        \n",
      "[MEM:508227584:3941535744] Fill: Ep.3:114, Score -167.294186, RewardAvg -0.164274, Used 290 of 131072        \n",
      "[MEM:508227584:3941535744] Fill: Ep.4:91, Score 26.297715, RewardAvg 0.027324, Used 381 of 131072          \n",
      "[MEM:508227584:3941535744] Fill: Ep.5:56, Score -91.314819, RewardAvg -0.090991, Used 437 of 131072        \n",
      "[MEM:508227584:3941535744] Fill: Ep.6:88, Score -252.655468, RewardAvg -0.247945, Used 525 of 131072        \n",
      "[MEM:508227584:3941535744] Fill: Ep.7:75, Score -104.780645, RewardAvg -0.103528, Used 600 of 131072        \n",
      "[MEM:508264448:3941797888] Fill: Ep.8:74, Score -179.637784, RewardAvg -0.177484, Used 674 of 131072        \n",
      "[MEM:508264448:3941797888] Fill: Ep.9:83, Score -115.128642, RewardAvg -0.114426, Used 757 of 131072        \n",
      "[MEM:508403712:3941535744] Fill: Ep.10:78, Score -156.669822, RewardAvg -0.154942, Used 835 of 131072        \n",
      "[MEM:508407808:3941535744] Fill: Ep.11:95, Score -169.077594, RewardAvg -0.165003, Used 930 of 131072        \n",
      "[MEM:508411904:3941535744] Fill: Ep.12:66, Score -137.614052, RewardAvg -0.136400, Used 996 of 131072        \n",
      "[MEM:508411904:3941797888] Fill: Ep.13:59, Score -99.980853, RewardAvg -0.099126, Used 1055 of 131072        \n",
      "\n",
      "Status: Used 1055 of 131072 from 0 to 1055\n",
      "[MEM:518275072:3947253760] Fill: Ep.14:104, Score -280.131546, RewardAvg -0.274479, Used 1159 of 131072        \n",
      "[MEM:549699584:3953397760] Fill: Ep.15:16, Score -3.065823, RewardAvg -0.003065, Used 1175 of 131072        \n",
      "Keyboard Interrupt\n"
     ]
    }
   ],
   "source": [
    "# Создать среду\n",
    "oEnvironment = CustomEnvironment(bRender=bRender)\n",
    "# Определить активный граф\n",
    "with tfGraph() as tfoGraph:\n",
    "    # Создать нейросеть `Актёр-цель`\n",
    "    nnActor = CreateActor(oEnvironment, 1)\n",
    "    # Создать виртуального игрока\n",
    "    oRandomPlayer = CustomPlayer(oEnvironment, nnActor, fnSelectNoisyRandom)\n",
    "    # Создать агента алгоритма `Soft Actor Critic`\n",
    "    oAgent = SacAgent(oRandomPlayer,\n",
    "        uReplayCapacity=uReplayCapacity,\n",
    "        fRatingDiscountFactor=fRatingDiscountFactor,\n",
    "        uBatchSize=uBatchSize,\n",
    "        bEpisodeRating=bEpisodeRating,\n",
    "        bPriorityMode=bPriorityMode,\n",
    "        fTrainableTargetAverageUpdateCoef=fTrainableTargetAverageUpdateCoef,\n",
    "        uLogLevel=uLogLevel,\n",
    "        sLogsPath='logs/' + sName + '/',\n",
    "        sRestorePath='models/' + sName + '/')\n",
    "\n",
    "    # Начать сессию вычисления графа\n",
    "    with tfSession(tfoGraph):\n",
    "        # Инициализировать глобальные переменные\n",
    "        tfInitGlobal()\n",
    "        # Инициализировать глобальные переменные\n",
    "        tfInitLocal()\n",
    "        # Инициализировать агента\n",
    "        oAgent.initialize()\n",
    "        if not bRestore or not oAgent.restore():\n",
    "            # Предварительно заполнениь буфер повтора начальными данными\n",
    "            oAgent.fill(uBatchSize * 4, uDebugLevel=2, bPrefill=True)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                # Заполнить буфер повтора шагом или эпизодом, в зависимости от настройки\n",
    "                uFillCount, bDone = oAgent.fill(1, uDebugLevel=1)\n",
    "                # На каждый новый шаг в буфре повтора выполнить тренировку\n",
    "                while uFillCount > 0:\n",
    "                    fTotalLoss = oAgent.train()\n",
    "                    uFillCount -= 1\n",
    "                # Сохранить модель\n",
    "                if bDone:\n",
    "                    oAgent.save()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nKeyboard Interrupt')\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Опустошить все буферы записи перед закрытием\n",
    "            oAgent.flush()\n",
    "            # Закрыть среду\n",
    "            oEnvironment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
