{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice\n",
    "\n",
    "Original work Copyright (c) June 2020, Sergei Sovik <sergeisovik@yahoo.com>\n",
    "\n",
    "Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.\n",
    "\n",
    "The software is provided `as is` and the author disclaims all warranties with regard to this software including all implied warranties of merchantability and fitness. In no event shall the author be liable for any special, direct, indirect, or consequential damages or any damages whatsoever resulting from loss of use, data or profits, whether in an action of contract, negligence or other tortious action, arising out of or in connection with the use or performance of this software.\n",
    "\n",
    "# Foreword\n",
    "The algorithm had to be implemented on old Tensorflow functions with Eager disabled, due to the fact that new functions of Tensorflow 2.2 led to a large memory leak and 32 GB of my memory were consumed in literally one hour.\n",
    "\n",
    "All names of variables and classes are given based on programming experience for more than 25 years, possibly unusual, but intuitive for readers and not a generally accepted standard.\n",
    "\n",
    "This article was written with the aim of expanding the circle of users with the `Soft Actor Critic` algorithm, since at the time of writing this article, it is the best, and all existing articles are written in a language incomprehensible to many programmers.\n",
    "\n",
    "For those who absolutely do not understand what a `Computional Graph` is and does not want to go into details. This is a model describing the relationship between all calculations, including determining their order of execution. Each operation is called a `Node`. A `Computional Graph` is somewhat reminiscent of a block diagram with many possible inputs and outputs. Thus, requesting to calculate the result of the `Node` from the neural network engine, all dependencies are calculated, and if necessary, input data is requested.\n",
    "\n",
    "# Algorithm `Genetic Soft Actor Critic`\n",
    "The algorithm is implemented as a single graph, which allows to reduce the amount of data exchange with the GPU, and speed up the learning process.\n",
    "\n",
    "The algorithm consists of four main blocks:\n",
    "- Block `Neural Network`\n",
    "- Block `Player`\n",
    "- Block `Genetic Replay Buffer`\n",
    "- Block `Trainer`\n",
    "\n",
    "Each of which can work in parallel-serial.\n",
    "\n",
    "Block `Neural network`\n",
    "A neural network consists of several independently trained blocks:\n",
    "- Two subnet clones `Trainer Actor` and `Target Actor`\n",
    "- Two duplicate subnets `Trainer Critic`\n",
    "- Two subnets `Target Critic`\n",
    "- Coefficient `Alpha Regulator`\n",
    "\n",
    "### Two subnet clones `Trainer Actor` and `Target Actor`\n",
    "The `Target Actor` is used exclusively for the ability to parallelize the training and filling in the `Replay Buffer` with new data and is a complete copy of the `Trainer Actor` neural network.\n",
    "\n",
    "### Two duplicate subnets `Trainer Critic`\n",
    "Necessary to minimize errors.\n",
    "\n",
    "### Two subnets `Target Critic`\n",
    "Used for smooth learning using the moving average method.\n",
    "\n",
    "### Coefficient `Alpha Regulator`\n",
    "Performs the role of micro-adjustment of the learning process, to increase accuracy.\n",
    "\n",
    "## Block `Player`\n",
    "There is a certain environment in which it is necessary to carry out certain actions to achieve the goal. To simplify understanding, let's call the environment `Game`. The task of the `Player` is to collect observation data from the `Game`, to perform actions, and to receive a `Reward` from the `Game` or to independently make a `Rating` of these actions. Every completed action is a step. In one step, we have the following data set: `Previous Observation`, `Current Observation`, `Completed Action`, `Reward` or `Rating`, `End status`. The decision about which action to take is made by the `Target Actor` network based on the data of the `Previous Observation`. If the decision leads to a situation that can be considered the end, the `Player` completes and resets the` Game`.\n",
    "\n",
    "There is two types of ratings:\n",
    "- Rate rewards for every step\n",
    "- Rate of the entire episode\n",
    "\n",
    "Each step is stored in the `Replay Buffer` for further training and is called the `Trajectory`\n",
    "\n",
    "### Rate rewards for every step\n",
    "The `Player` takes an action and immediately writes to the `Replay Buffer` the following indicators: `Previous Observation`, `Current Observation`, `Completed Action`, `Reward`. Then `Rating` produced by `Trainer`.\n",
    "\n",
    "### Rate of the entire episode\n",
    "The `Player` takes action and stores to the `Temporary Buffer`. At the end of the episode, it calculates the `Rating` at each step and then stores to the `Replay Buffer` of the entire episode with the following indicators: `Previous Observation`, `Current Observation`, `Completed Action`, ` Rating`.\n",
    "\n",
    "## Block `Genetic Replay Buffer`\n",
    "It is a cyclic `Repeat Buffer`, which, when overflowed, starts overwriting older data with newer ones. It also includes the `Tree-based buffer of the sum` and the` Tree-based buffer of the maximum` used to calculate the priority of each step stored in the `Replay Buffer`. The tree-based buffers in a pair have the similarity with the genetic algorithm when selecting data from the `Replay Buffer`, which can greatly accelerate the learning process, and also reduces the likelihood of knocking down or freezing of the trained model in poor condition. A poor condition can be the result of the neural network getting used to poor results.\n",
    "\n",
    "## Block `Trainer`\n",
    "The main brain center of the algorithm that controls all the other blocks.\n",
    "\n",
    "The training cycle for each step of the `Player`:\n",
    "1. Select a batch of `Trajectories` from the `Genetic Replay Buffer`, taking into account the priorities.\n",
    "2. Train two `Trainer Critics` independently.\n",
    "3. Update the `Target Critics` using the `Moving Average` method.\n",
    "4. Train `Trainer Actor` and `Alpha Regulator`.\n",
    "5. Update the `Target Actor`.\n",
    "6. Update priorities in the `Genetic Replay Buffer` for processed steps from a batch.\n",
    "\n",
    "The learning process is standard: forward distribution, loss calculation, gradient calculation, back propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example results of training `LunarLander-v2`\n",
    "<table style=\"float:left;\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: center;\">Average Score per Episode</td>\n",
    "        <td style=\"text-align: center;\">Average Steps per Episode</td>\n",
    "    </tr><tr>\n",
    "        <td><img src=\"GSAC-Score.svg\" width=\"320pt\"></td>\n",
    "        <td><img src=\"GSAC-Steps.svg\" width=\"320pt\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[box2d]\n",
    "!pip install Box2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:24.757385Z",
     "start_time": "2020-06-29T17:51:24.753978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use GPU?\n",
    "bGPU = False\n",
    "# Size of single layer of neural network `Encoder`\n",
    "uEncoderLayerSize = 64\n",
    "# Model and log name\n",
    "sName = \"%d\" % uEncoderLayerSize\n",
    "# Limit maximum episode steps long\n",
    "uEpisodeStepLimit = 1024\n",
    "# Size of replay buffer, must contain at least 100 episodes\n",
    "uReplayCapacity = 128 * 1024\n",
    "# Batch size of training data\n",
    "uBatchSize = 256\n",
    "# Restore graph from checkpoint?\n",
    "bRestore = False\n",
    "# Do on screen render?\n",
    "bRender = False\n",
    "# Rate the entire episode?\n",
    "bEpisodeRating = False\n",
    "# Prioritize replay buffer?\n",
    "bPriorityMode = False\n",
    "# Factor of discounting rating\n",
    "fRatingDiscountFactor = 0.99\n",
    "# Update coefficient of target neural network\n",
    "fTrainableTargetAverageUpdateCoef = 0.005\n",
    "# Logging and statistics level\n",
    "uLogLevel = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare workspace\n",
    "Import modules and configure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:37.187031Z",
     "start_time": "2020-06-29T17:51:25.911838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Disable Tensorflow console spam\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "# Disable GPU for small networks, CPU is faster\n",
    "if not bGPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow._api.v2.compat.v1 as tf1\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.initializers as ki\n",
    "\n",
    "# Turn off the allocation of all available memory\n",
    "if bGPU:\n",
    "    aoGPUPhysicalList = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if aoGPUPhysicalList:\n",
    "        try:\n",
    "            for oGPUDevice in aoGPUPhysicalList:\n",
    "                tf.config.experimental.set_memory_growth(oGPUDevice, True)\n",
    "        except RuntimeError as e:\n",
    "            pass\n",
    "\n",
    "# Set default type for keras networks\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# Manual control of graph build and session control, to remove memory leaking problems\n",
    "tf1.disable_eager_execution()\n",
    "\n",
    "# Date and time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Virtual game environment for `Reninforcement learning`\n",
    "import gym\n",
    "\n",
    "# Abstract methods support\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:37.396745Z",
     "start_time": "2020-06-29T17:51:37.241120Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Get current process memory usage\n",
    "def getMemoryUsage():\n",
    "    oProcess = psutil.Process(os.getpid())\n",
    "    return \"%d:%d\" % (oProcess.memory_info().rss, oProcess.memory_info().vms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow extension\n",
    "Set of functions and classes for simple work with _Tensorflow_ graphs and sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:40.269019Z",
     "start_time": "2020-06-29T17:51:40.236395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default types\n",
    "tfFloat = tf.keras.backend.floatx()\n",
    "tfInt = tf.int32\n",
    "\n",
    "# Graph array variable initializer (for internal usage only)\n",
    "class ArrayInitializer(ki.Initializer):\n",
    "    def __init__(self, aValue):\n",
    "        super(ArrayInitializer, self).__init__()\n",
    "        self.aValue = aValue\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return tf.convert_to_tensor(self.aValue, dtype=dtype)\n",
    "\n",
    "# Create initializer defined by shape and value (for internal usage only)\n",
    "def tfInitializer(tShape, oValue=None):\n",
    "    if (oValue is not None) and (not isinstance(oValue, ki.Initializer)):\n",
    "        if isinstance(oValue, list):\n",
    "            oValue = np.array(oValue)\n",
    "        if type(oValue) is np.ndarray:\n",
    "            if oValue.shape != tShape:\n",
    "                raise TypeError(\"Wrong initializer shape %s, must be %s\" % (oValue.shape, tShape))\n",
    "            return ArrayInitializer(oValue)\n",
    "        elif oValue == 0:\n",
    "            return ki.Zeros()\n",
    "        elif oValue == 1:\n",
    "            return ki.Ones()\n",
    "        else:\n",
    "            return ki.Constant(oValue)\n",
    "    return oValue\n",
    "\n",
    "# Define local variable, used inside one function block of graph\n",
    "def tfLocalVariable(sName, oType, oValue=None, bTrainable=False):\n",
    "    if isinstance(oType, tuple):\n",
    "        sType, tShape = oType[0], oType[1]\n",
    "    else:\n",
    "        sType, tShape = oType, []\n",
    "    oValue = tfInitializer(tShape, oValue)\n",
    "    return tf1.get_local_variable(sName, shape=tShape, dtype=sType, initializer=oValue, trainable=bTrainable, use_resource=True)\n",
    "\n",
    "# Define global variable, used within all graph inside one session\n",
    "def tfGlobalVariable(sName, oType, oValue=None, bTrainable=False):\n",
    "    if isinstance(oType, tuple):\n",
    "        sType, tShape = oType[0], oType[1]\n",
    "    else:\n",
    "        sType, tShape = oType, []\n",
    "    oValue = tfInitializer(tShape, oValue)\n",
    "    return tf1.get_variable(sName, shape=tShape, dtype=sType, initializer=oValue, trainable=bTrainable, use_resource=True)\n",
    "\n",
    "# Define constant\n",
    "def tfConstant(sName, oType, oValue):\n",
    "    sType = oType[0] if isinstance(oType, tuple) else oType\n",
    "    return tf1.constant(oValue, dtype=sType, name=sName)\n",
    "\n",
    "# Define graph input data node\n",
    "def tfInput(sName, oType):\n",
    "    if isinstance(oType, tuple):\n",
    "        sType, tShape = oType[0], oType[1]\n",
    "    else:\n",
    "        sType, tShape = oType, []\n",
    "    return tf1.placeholder(sType, shape=tShape, name=sName)\n",
    "\n",
    "# Alias for a function of waiting to complete list of operations \n",
    "class tfWait(object):\n",
    "    def __init__(self, aDependencies):\n",
    "        self.tfControl = tf.control_dependencies(aDependencies)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.tfControl.__enter__()\n",
    "\n",
    "    def __exit__(self, clsErrorType, errValue, oTraceback):\n",
    "        return self.tfControl.__exit__(clsErrorType, errValue, oTraceback)\n",
    "\n",
    "# Current active session\n",
    "tfActiveSession = None\n",
    "\n",
    "# Create session for graph execution control\n",
    "class tfSession(object):\n",
    "    def __init__(self, tfoGraph=None, sTarget=''):\n",
    "        self.tfoSession = tf1.Session(target=sTarget, graph=tfoGraph.tfoGraph)\n",
    "\n",
    "    def __enter__(self):\n",
    "        global tfActiveSession\n",
    "        self.tfoDefault = self.tfoSession.as_default()\n",
    "        self.tfoDefault.__enter__()\n",
    "        self.tfRestoreSession = tfActiveSession\n",
    "        tfActiveSession = self\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, clsErrorType, errValue, oTraceback):\n",
    "        global tfActiveSession\n",
    "        self.tfoSession.close()\n",
    "        self.tfoDefault = None\n",
    "        tfActiveSession = self.tfRestoreSession\n",
    "\n",
    "    def initGlobal(self):\n",
    "        self.tfoSession.run(tf1.global_variables_initializer())\n",
    "\n",
    "    def initLocal(self):\n",
    "        self.tfoSession.run(tf1.local_variables_initializer())\n",
    "\n",
    "    def eval(self, tfoOutputTensor, oInputDictionary=None):\n",
    "        return self.tfoSession.run(tfoOutputTensor, oInputDictionary)\n",
    "\n",
    "# Evaluate result of graph node inside current session\n",
    "def tfEval(tfoOutputTensor, oInputDictionary=None):\n",
    "    return tfActiveSession.eval(tfoOutputTensor, oInputDictionary)\n",
    "\n",
    "# Initialize global variables inside current session\n",
    "def tfInitGlobal():\n",
    "    tfActiveSession.initGlobal()\n",
    "\n",
    "# Initialize local variables inside current session\n",
    "def tfInitLocal():\n",
    "    tfActiveSession.initLocal()\n",
    "\n",
    "# Current active graph\n",
    "tfActiveGraph = None\n",
    "\n",
    "# Create graph\n",
    "class tfGraph(object):\n",
    "    def __init__(self):\n",
    "        self.tfoGraph = tf.Graph()\n",
    "        self.tfoDefault = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        global tfActiveGraph\n",
    "        self.tfoDefault = self.tfoGraph.as_default()\n",
    "        self.tfoDefault.__enter__()\n",
    "        self.tfRestoreGraph = tfActiveGraph\n",
    "        tfActiveGraph = self\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, clsErrorType, errValue, oTraceback):\n",
    "        global tfActiveGraph\n",
    "        self.tfoDefault = None\n",
    "        tfActiveGraph = self.tfRestoreGraph\n",
    "        \n",
    "# Alias for optimizer\n",
    "class AMSgrad(ko.Adam):\n",
    "    def __init__(self, lr=0.001):\n",
    "        super(AMSgrad, self).__init__(lr=lr, amsgrad=True)\n",
    "\n",
    "# Clip gradient values to avoid `inf` and `nan`\n",
    "def fnClipGradients(aGradients, tfMaxNormal):\n",
    "    aClippedGradients = []\n",
    "    for tfoGrad in aGradients:\n",
    "        if tfoGrad is not None:\n",
    "            if isinstance(tfoGrad, tf.IndexedSlices):\n",
    "                tfoTemp = tf.clip_by_norm(tfoGrad.values, tfMaxNormal)\n",
    "                tfoGrad = tf.IndexedSlices(tfoTemp, tfoGrad.indices, tfoGrad.dense_shape)\n",
    "            else:\n",
    "                tfoGrad = tf.clip_by_norm(tfoGrad, tfMaxNormal)\n",
    "        aClippedGradients.append(tfoGrad)\n",
    "    return aClippedGradients\n",
    "\n",
    "# Copy of weights from one neuron network to another\n",
    "def fnHardUpdate(tafTargetVariables, tafSourceVariables):\n",
    "    atopUpdates = []\n",
    "    tfStrategy = tf.distribute.get_strategy()\n",
    "\n",
    "    for (tfoTarget, tfoSource) in zip(tafTargetVariables, tafSourceVariables):\n",
    "        def fnUpdate(tfoTarget, tfoSource):\n",
    "            return tfoTarget.assign(tfoSource)\n",
    "\n",
    "        if tf.distribute.has_strategy() and tfoTarget.trainable:\n",
    "            topUpdate = tfStrategy.extended.update(tfoTarget, fnUpdate, args=(tfoSource,))\n",
    "        else:\n",
    "            topUpdate = fnUpdate(tfoTarget, tfoSource)\n",
    "\n",
    "        atopUpdates.append(topUpdate)\n",
    "    return tf.group(*atopUpdates)\n",
    "\n",
    "# Copy of weights from one neuron network to another using `moving average` method\n",
    "def fnSoftUpdate(tafTargetVariables, tafSourceVariables, tfZero, tfOne, tfTrainableTargetAverageForgetCoef, tfTrainableTargetAverageUpdateCoef):\n",
    "    atopUpdates = []\n",
    "    tfStrategy = tf.distribute.get_strategy()\n",
    "\n",
    "    for (tfoTarget, tfoSource) in zip(tafTargetVariables, tafSourceVariables):\n",
    "        def fnUpdate(tfoTarget, tfoSource):\n",
    "            if not tfoTarget.trainable:\n",
    "                tfTargetAverageForgetCoef = tfZero\n",
    "                tfTargetAverageUpdateCoef = tfOne\n",
    "            else:\n",
    "                tfTargetAverageForgetCoef = tfTrainableTargetAverageForgetCoef\n",
    "                tfTargetAverageUpdateCoef = tfTrainableTargetAverageUpdateCoef\n",
    "\n",
    "            return tfoTarget.assign(tfoTarget * tfTargetAverageForgetCoef + tfoSource * tfTargetAverageUpdateCoef)\n",
    "\n",
    "        if tf.distribute.has_strategy() and tfoTarget.trainable:\n",
    "            topUpdate = tfStrategy.extended.update(tfoTarget, fnUpdate, args=(tfoSource,))\n",
    "        else:\n",
    "            topUpdate = fnUpdate(tfoTarget, tfoSource)\n",
    "\n",
    "        atopUpdates.append(topUpdate)\n",
    "    return tf.group(*atopUpdates)\n",
    "\n",
    "# Write detailed summary of tensor, one graph variable\n",
    "def fnTensorSummary(oSummaryWriter, sTag, tfoVariable, tuStep):\n",
    "    with oSummaryWriter.as_default():\n",
    "        with tf.name_scope(sTag):\n",
    "            return tf.group(\n",
    "                tf.summary.histogram('Histogram', tfoVariable, tuStep),\n",
    "                tf.summary.scalar('Mean', tf.reduce_mean(tfoVariable, 'fMean'), tuStep),\n",
    "                tf.summary.scalar('MeanAbs', tf.reduce_mean(tf.abs(tfoVariable), 'fMeanAbs'), tuStep),\n",
    "                tf.summary.scalar('Max', tf.reduce_max(tfoVariable), tuStep),\n",
    "                tf.summary.scalar('Min', tf.reduce_min(tfoVariable), tuStep)\n",
    "            )\n",
    "\n",
    "# Write detailed summary of neural network weights\n",
    "def fnWeightsSummary(oSummaryWriter, zipWeightfoGradientsAndVariables, tuStep):\n",
    "    aOps = []\n",
    "    with oSummaryWriter.as_default():\n",
    "        for tfaGradientsGroup, tfaVariablesGroup in zipWeightfoGradientsAndVariables:\n",
    "            sGroupName = tfaVariablesGroup.name.replace(':', '_')\n",
    "\n",
    "            if isinstance(tfaVariablesGroup, tf.IndexedSlices):\n",
    "                tfaValues = tfaVariablesGroup.values\n",
    "            else:\n",
    "                tfaValues = tfaVariablesGroup\n",
    "            aOps.append(tf.summary.histogram('Weights/' + sGroupName, tfaValues, tuStep))\n",
    "            aOps.append(tf.summary.scalar('WeightsNorm/' + sGroupName, tf.linalg.global_norm([tfaValues]), tuStep))\n",
    "\n",
    "            if tfaGradientsGroup is not None:\n",
    "                if isinstance(tfaGradientsGroup, tf.IndexedSlices):\n",
    "                    tfaGradients = tfaGradientsGroup.values\n",
    "                else:\n",
    "                    tfaGradients = tfaGradientsGroup\n",
    "                aOps.append(tf.summary.histogram('Gradients/' + sGroupName, tfaGradients, tuStep))\n",
    "                aOps.append(tf.summary.scalar('GradientsNorm/' + sGroupName, tf.linalg.global_norm([tfaGradients]), tuStep))\n",
    "    return tf.group(*aOps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set of functions to select action inside discrete models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:41.687319Z",
     "start_time": "2020-06-29T17:51:41.680512Z"
    }
   },
   "outputs": [],
   "source": [
    "def fnSelectBest(tafUnnormalizedLogProbabilities):\n",
    "    return tf.argmax(tafUnnormalizedLogProbabilities, axis=-1, output_type=tfInt)\n",
    "\n",
    "def fnSelectRandom(tafUnnormalizedLogProbabilities):\n",
    "    return tf.squeeze(tf.random.categorical(tafUnnormalizedLogProbabilities, 1, dtype=tfInt), axis=-1)\n",
    "\n",
    "def fnSelectNoisyBest(tafUnnormalizedLogProbabilities):\n",
    "    with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "        tfTotalMinLogInput = tfConstant('fMinLogInput', tfFloat, 1e-8)\n",
    "        tfTotalMaxLogInput = tfConstant('fMaxLogInput', tfFloat, 1-1e-8)\n",
    "\n",
    "    with tf.name_scope('fnSelectNoisyBest'):\n",
    "        tafRandomUniform = tf.random.uniform(tafUnnormalizedLogProbabilities.shape, minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "        tafGumbel = -tf.math.log(-tf.math.log(tafRandomUniform)) # pylint: disable=invalid-unary-operand-type\n",
    "        tafUnnormalizedNoisyLogProbabilities = tafUnnormalizedLogProbabilities + tafGumbel\n",
    "\n",
    "    return tf.argmax(tafUnnormalizedNoisyLogProbabilities, axis=-1, output_type=tfInt)\n",
    "\n",
    "def fnSelectNoisyRandom(tafUnnormalizedLogProbabilities):\n",
    "    with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "        tfTotalMinLogInput = tfConstant('fMinLogInput', tfFloat, 1e-8)\n",
    "        tfTotalMaxLogInput = tfConstant('fMaxLogInput', tfFloat, 1-1e-8)\n",
    "\n",
    "    with tf.name_scope('fnSelectNoisyRandom'):\n",
    "        tafRandomUniform = tf.random.uniform(tafUnnormalizedLogProbabilities.shape, minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "        tafGumbel = -tf.math.log(-tf.math.log(tafRandomUniform)) # pylint: disable=invalid-unary-operand-type\n",
    "        tafUnnormalizedNoisyLogProbabilities = tafUnnormalizedLogProbabilities + tafGumbel\n",
    "\n",
    "    return tf.squeeze(tf.random.categorical(tafUnnormalizedNoisyLogProbabilities, 1, dtype=tfInt), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment model\n",
    "The environment model determines the current observation state, and on the basis of control commands, it calculates the next observation state, the reward for the completed action, and also informs about the end, i.e. the need to reset the environment to its initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:42.965560Z",
     "start_time": "2020-06-29T17:51:42.962374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base environment class\n",
    "class EnvironmentImpl(object):\n",
    "    def __init__(self, uObservationSize, uActionsSize, bDiscrete):\n",
    "        # Size of observation array\n",
    "        self.uObservationSize = uObservationSize\n",
    "        # Size of actions array\n",
    "        self.uActionsSize = uActionsSize\n",
    "        # Discrete or continuous environment control\n",
    "        self.bDiscrete = bDiscrete\n",
    "\n",
    "    # Reset environment. Returns current observation after reset\n",
    "    @abc.abstractmethod\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    # Next step of observation state. Returns new observation, reward, and finish state\n",
    "    @abc.abstractmethod\n",
    "    def step(self, oAction):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:44.209273Z",
     "start_time": "2020-06-29T17:51:44.202193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Environment model for game LunarLander\n",
    "class CustomEnvironment(EnvironmentImpl):\n",
    "    def __init__(self, bRender=True):\n",
    "        sEnvironment = 'LunarLander-v2'\n",
    "        self.oEnvironment = gym.make(sEnvironment)\n",
    "        \n",
    "        # Maximum one episode duration\n",
    "        # Must be at least 4 times greater of average episode duration, but not to big\n",
    "        self.oEnvironment._max_episode_steps = uEpisodeStepLimit * 10\n",
    "        \n",
    "        tObservationShape = self.oEnvironment.observation_space.shape\n",
    "        tActionsShape = (self.oEnvironment.action_space.n,)\n",
    "\n",
    "        super(CustomEnvironment, self).__init__(\n",
    "            np.prod(list(tObservationShape)),\n",
    "            np.prod(list(tActionsShape)),\n",
    "            True\n",
    "        )\n",
    "\n",
    "        self.bRender = bRender\n",
    "\n",
    "    def reset(self):\n",
    "        return self.oEnvironment.reset()\n",
    "\n",
    "    def step(self, uAction):\n",
    "        aObservation, fReward, bDone, _ = self.oEnvironment.step(uAction)\n",
    "\n",
    "        if self.bRender:\n",
    "            self.oEnvironment.render()\n",
    "\n",
    "        oInfo = {}\n",
    "        return aObservation, fReward, bDone, oInfo\n",
    "\n",
    "    def close(self):\n",
    "        self.oEnvironment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:46.093768Z",
     "start_time": "2020-06-29T17:51:46.079441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base neural network model\n",
    "class ModelImpl(tf.keras.Model):\n",
    "    def __init__(self, sName=None):\n",
    "        super(ModelImpl, self).__init__(name=sName)\n",
    "\n",
    "    # Override method to make Tensorflow define `self._build_input_shape`\n",
    "    #  `self._build_input_shape` used for logging of neural network model structure \n",
    "    def build(self, tInputShape):\n",
    "        super(ModelImpl, self).build(tInputShape)\n",
    "\n",
    "# WARNING! Do not use `relu`, to prevent `vanishing gradient` problem, when value is close or equal zero\n",
    "\n",
    "# Neural network `Critic`, assesses the potential benefits of the environment observations with actions\n",
    "class CriticNetwork(ModelImpl):\n",
    "    def __init__(self, sName=None):\n",
    "        super(CriticNetwork, self).__init__(sName=sName)\n",
    "        # `Concatenator` block\n",
    "        self.fnFlattenObservations = kl.Flatten()\n",
    "        self.fnFlattenActions = kl.Flatten()\n",
    "        self.fnConcatInput = kl.Concatenate()\n",
    "        # `Encoder` block\n",
    "        self.fnEncoder = tf.keras.Sequential()\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(32, activation='elu'))\n",
    "        # `Critic` block\n",
    "        self.fnCritic = kl.Dense(1, activation='linear')\n",
    "\n",
    "    # Create weights variables for neural network\n",
    "    def build(self, tObservationShape, tActionsShape):\n",
    "        super(CriticNetwork, self).build((tObservationShape[0], np.prod(list(tObservationShape[1:])) + np.prod(list(tActionsShape[1:]))))\n",
    "\n",
    "    # Prepare data using `Concatenator` block for getting result of neural network\n",
    "    def prepare(self, tafObservations, tafActions):\n",
    "        tafFlatObservations = self.fnFlattenObservations.call(tafObservations)\n",
    "        tafFlatActions = self.fnFlattenActions.call(tafActions)\n",
    "        tafStates = self.fnConcatInput([tafFlatObservations, tafFlatActions])\n",
    "        return tafStates\n",
    "\n",
    "    # Calculate result of neural network (predict rating)\n",
    "    def call(self, tafStates):\n",
    "        tafEncoded = self.fnEncoder(tafStates)\n",
    "        tafPredRating = self.fnCritic(tafEncoded)\n",
    "        return tafPredRating\n",
    "\n",
    "# Neural network `Discrete actor`, generates unnromalized action log probabilities\n",
    "class DiscreteActorNetwork(tf.keras.Sequential):\n",
    "    def __init__(self, uActionCount, sName=None):\n",
    "        super(DiscreteActorNetwork, self).__init__(name=sName)\n",
    "        # `Encoder` block\n",
    "        self.fnEncoder = tf.keras.Sequential()\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(32, activation='elu'))\n",
    "        # `Actor` block\n",
    "        self.fnAction = kl.Dense(uActionCount, activation='linear')\n",
    "        # Sequience of calculations\n",
    "        self.add(self.fnEncoder)\n",
    "        self.add(self.fnAction)\n",
    "\n",
    "# Neural network `Continous actor`, generates mean and standard deviation of possible actions\n",
    "class ContinuousActorNetwork(ModelImpl):\n",
    "    def __init__(self, uActionCount, sName=None):\n",
    "        super(ContinuousActorNetwork, self).__init__(sName=sName)\n",
    "        # `Encoder` block\n",
    "        self.fnEncoder = tf.keras.Sequential()\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(uEncoderLayerSize, activation='elu'))\n",
    "        self.fnEncoder.add(kl.Dense(32, activation='elu'))\n",
    "        # `Actor` block\n",
    "        # The initial values are set around zero with a slight deviation, to avoid sudden and unexpected actions\n",
    "        self.fnMean = kl.Dense(uActionCount, activation='linear',\n",
    "            kernel_initializer=ki.VarianceScaling(0.1),\n",
    "            bias_initializer=ki.Zeros(),)\n",
    "        self.fnStd = kl.Dense(uActionCount, activation='linear',\n",
    "            kernel_initializer=ki.VarianceScaling(0.1),\n",
    "            bias_initializer=ki.Constant(0.0),)\n",
    "\n",
    "    # Create weights variables for neural network\n",
    "    def build(self, tObservationShape):\n",
    "        super(ContinuousActorNetwork, self).build(tObservationShape)\n",
    "\n",
    "    # Calculate result of neural network\n",
    "    def call(self, tafState):\n",
    "        tafEncoded = self.fnEncoder(tafState)\n",
    "        tafPredLocation = self.fnMean(tafEncoded)\n",
    "        tafPredScale = self.fnStd(tafEncoded)\n",
    "        return tafPredLocation, tafPredScale\n",
    "\n",
    "# Create neural network `Actor` depending on the type of environment\n",
    "def CreateActor(oEnvironment, uBatchSize=256, sName='Actor'):\n",
    "    if oEnvironment.bDiscrete:\n",
    "        nnActor = DiscreteActorNetwork(oEnvironment.uActionsSize, sName)\n",
    "    else:\n",
    "        nnActor = ContinuousActorNetwork(oEnvironment.uActionsSize, sName)\n",
    "    nnActor.build((uBatchSize, oEnvironment.uObservationSize))\n",
    "    return nnActor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:47.555587Z",
     "start_time": "2020-06-29T17:51:47.544253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class `Player` used to control environment via getting actions from neural network `Actor`\n",
    "# fnPolicy - function to convert log probabilities into action id\n",
    "class CustomPlayer(object):\n",
    "    def __init__(self, oEnvironment, nnActor, fnPolicy=fnSelectBest):\n",
    "        self.oEnvironment = oEnvironment\n",
    "        self.nnActor = nnActor\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        # Constants used with graph\n",
    "        with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "            tuActionsSize = tfConstant('uActionsSize', tfInt, self.oEnvironment.uActionsSize)\n",
    "            tuZero = tfConstant('uZero', tfInt, 0)\n",
    "\n",
    "        # Construct block of graph to compute actions\n",
    "        with tf.name_scope('CustomPlayer'):\n",
    "            # The `fnAction` function block\n",
    "            with tf.name_scope('fnAction'):\n",
    "                with tf1.variable_scope('Input', reuse=tf1.AUTO_REUSE):\n",
    "                    # Input data node of environment observation\n",
    "                    self.tinafReplayObservation = tfInput('afObservation', (tfFloat, [self.oEnvironment.uObservationSize]))\n",
    "\n",
    "                # Calculate unnormalized log probabilities of next action\n",
    "                tafActions = nnActor.call(self.tinafReplayObservation[None, :], training=False)\n",
    "                self.tafActions = tf.squeeze(tafActions)\n",
    "                # Convert probabilities into action using policy function\n",
    "                self.tuAction = tf.squeeze(tf.clip_by_value(fnPolicy(tafActions), tuZero, tuActionsSize))\n",
    "\n",
    "    # Calculate next action\n",
    "    def action(self, afObservation):\n",
    "        return tfEval([self.tafActions, self.tuAction], {self.tinafReplayObservation: afObservation})\n",
    "\n",
    "    # Reset environment and observation state\n",
    "    def reset(self):\n",
    "        afObservation = self.oEnvironment.reset()\n",
    "        self.afPrevObservation = None\n",
    "        self.uAction = None\n",
    "        self.afActions = None\n",
    "        self.afObservation = np.array(afObservation, dtype=tfFloat).flatten()\n",
    "        self.uStep = 0\n",
    "        self.fScore = 0\n",
    "        self.fReward = 0\n",
    "        self.fAverageReward = 0\n",
    "        self.bDone = False\n",
    "\n",
    "    # Take the next intended action\n",
    "    def next(self):\n",
    "        if not self.bDone:\n",
    "            self.afActions, self.uAction = self.action(self.afObservation)\n",
    "\n",
    "            if self.uStep >= uEpisodeStepLimit:\n",
    "                self.uAction = 0\n",
    "            \n",
    "            afObservation, self.fReward, bDone, _ = self.oEnvironment.step(self.uAction)\n",
    "\n",
    "            self.afPrevObservation = self.afObservation\n",
    "            self.afObservation = np.array(afObservation, dtype=tfFloat).flatten()\n",
    "            self.uStep += 1\n",
    "            self.fScore += self.fReward\n",
    "            self.fAverageReward = self.fAverageReward * 0.999 + self.fReward * 0.001\n",
    "            if bDone:\n",
    "                self.bDone = True\n",
    "        return self.bDone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent `Soft Actor Critic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T17:51:50.976829Z",
     "start_time": "2020-06-29T17:51:50.803624Z"
    }
   },
   "outputs": [],
   "source": [
    "class SacAgent(object):\n",
    "    def __init__(self,\n",
    "            oPlayer,\n",
    "            uReplayCapacity=128*1024,\n",
    "            fRatingDiscountFactor=0.99,\n",
    "            uBatchSize=256,\n",
    "            bEpisodeRating=False,\n",
    "            bPriorityMode=True,\n",
    "            fTrainableTargetAverageUpdateCoef=0.005,\n",
    "            tfTrainStepCounter=None,\n",
    "            uLogLevel=1,\n",
    "            sLogsPath='logs/',\n",
    "            sRestorePath='models/'):\n",
    "\n",
    "        if uReplayCapacity < 2:\n",
    "            raise TypeError('uReplayCapacity must be greater 1, but given %d' % uReplayCapacity)\n",
    "\n",
    "        # Input params\n",
    "\n",
    "        self.oPlayer = oPlayer\n",
    "        self.uBatchSize = uBatchSize\n",
    "        self.bEpisodeRating = bEpisodeRating\n",
    "        self.bPriorityMode = bPriorityMode\n",
    "        self.fRatingDiscountFactor = fRatingDiscountFactor\n",
    "\n",
    "        # Coefficient of smoothing statistics of the average value of gradients\n",
    "        fGradientNormalUpdateCoef = 0.01\n",
    "        # Maximum mean gradients value\n",
    "        fMaxGradientNormal = 200\n",
    "\n",
    "        self.uLogLevel = uLogLevel\n",
    "\n",
    "        # Neural network optimizers\n",
    "\n",
    "        self.koActorOptimizer = AMSgrad(3e-4)\n",
    "        self.koCriticOptimizer = AMSgrad(3e-4)\n",
    "        self.koAlphaOptimizer = AMSgrad(3e-4)\n",
    "\n",
    "        # Summary writer\n",
    "\n",
    "        dtCurrentTime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        sLogPath = sLogsPath + dtCurrentTime\n",
    "        self.oSummaryWriter = tf.summary.create_file_writer(sLogPath)\n",
    "\n",
    "        # Temporary buffer for full episode\n",
    "\n",
    "        if self.bEpisodeRating:\n",
    "            # Current data size inside temporary buffer\n",
    "            self.uBufferSize = 0\n",
    "            # Current temporary buffer capacity\n",
    "            self.uBufferCapacity = 256\n",
    "\n",
    "            # Temporary buffer storage: observations, actions, rewards\n",
    "            self.afPrevObservations = np.zeros((self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize), dtype=tfFloat)\n",
    "            self.afObservations = np.zeros((self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize), dtype=tfFloat)\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                self.auActions = np.zeros((self.uBufferCapacity,), dtype=np.int32)\n",
    "            else:\n",
    "                self.afActions = np.zeros((self.uBufferCapacity, self.oPlayer.oEnvironment.uActionsSize), dtype=tfFloat)\n",
    "            self.afRewards = np.zeros((self.uBufferCapacity,), dtype=tfFloat)\n",
    "            self.afRatings = np.zeros((self.uBufferCapacity,), dtype=tfFloat)\n",
    "\n",
    "        # Lists of variables for storing on disk\n",
    "\n",
    "        aTrainVariables = []\n",
    "        aTargetVariables = oPlayer.nnActor.trainable_variables\n",
    "        aReplayBufferVariables = []\n",
    "\n",
    "        # Global constants\n",
    "\n",
    "        with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "            tfZero = tfConstant('fZero', tfFloat, 0)\n",
    "            tfHalf = tfConstant('fHalf', tfFloat, 0.5)\n",
    "            tfOne = tfConstant('fOne', tfFloat, 1)\n",
    "            tuOne = tfConstant('uOne', tfInt, 1)\n",
    "            if self.bPriorityMode:\n",
    "                tuTwo = tfConstant('uTwo', tfInt, 2)\n",
    "\n",
    "            tuActionsSize = tfConstant('uActionsSize', tfInt, oEnvironment.uActionsSize)\n",
    "            tuOne64 = tfConstant('uOne64', tf.int64, 1)\n",
    "\n",
    "            if fMaxGradientNormal is not None:\n",
    "                tfMaxGradientNormal = tfConstant('fMaxGradientNormal', tfFloat, fMaxGradientNormal)\n",
    "\n",
    "            if self.uLogLevel > 1:\n",
    "                tfGradientNormalUpdateCoef = tfConstant('fGradientNormalUpdateCoef', tfFloat, fGradientNormalUpdateCoef)\n",
    "                tfGradientNormalForgetCoef = tfConstant('fGradientNormalForgetCoef', tfFloat, 1.0 - fGradientNormalUpdateCoef)\n",
    "\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                tfTotalMinLogInput = tfConstant('fMinClip', tfFloat, 1e-8)\n",
    "                tfTotalMaxLogInput = tfConstant('fMaxClip', tfFloat, 1-1e-8)\n",
    "            else:\n",
    "                tfLogSqrtPi2 = tfConstant('fLogSqrtPi2', tfFloat, math.log(math.sqrt(math.pi * 2.0)))\n",
    "                tfTotalMinScale = tfConstant('fMinScale', tfFloat, -20)\n",
    "                tfTotalMaxScale = tfConstant('fMaxScale', tfFloat, 2)\n",
    "\n",
    "            tfTrainableTargetAverageUpdateCoef = tfConstant('fTrainableTargetAverageUpdateCoef', tfFloat, fTrainableTargetAverageUpdateCoef)\n",
    "            tfTrainableTargetAverageForgetCoef = tfConstant('fTrainableTargetAverageForgetCoef', tfFloat, 1.0 - fTrainableTargetAverageUpdateCoef)\n",
    "\n",
    "        with tf1.variable_scope('Var', reuse=tf1.AUTO_REUSE):\n",
    "            # Current episode\n",
    "            self.tuEpisode = tfGlobalVariable('uEpisode', tfInt, 1)\n",
    "            aTrainVariables.append(self.tuEpisode)\n",
    "\n",
    "        # Body of genetic replay buffer\n",
    "\n",
    "        with tf.name_scope('ReplayBuffer'):\n",
    "            with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "                # Cyclic buffer capacity\n",
    "                self.tuReplayCapacity = tfConstant('uCapacity', tfInt, uReplayCapacity)\n",
    "                # Cyclic buffer start offset\n",
    "                self.tuReplayStart = tfGlobalVariable('uStart', tfInt, 0)\n",
    "                aReplayBufferVariables.append(self.tuReplayStart)\n",
    "                # Cyclic buffer end offset\n",
    "                self.tuReplayEnd = tfGlobalVariable('uEnd', tfInt, 0)\n",
    "                aReplayBufferVariables.append(self.tuReplayEnd)\n",
    "\n",
    "                # Tree-based buffer params\n",
    "                if self.bPriorityMode:\n",
    "                    uTreeSize = int(math.pow(2, math.ceil(math.log2(uReplayCapacity))))\n",
    "                    tuReplayTreeSize = tfConstant('uTreeSize', tfInt, uTreeSize)\n",
    "                    tuHalfReplayTreeSize = tfConstant('uHalfTreeSize', tfInt, uTreeSize>>1)\n",
    "                    tauOne = tfConstant('auOne', (tfInt, [1]), [1])\n",
    "\n",
    "                    # Coefficients to calculate priority\n",
    "                    # `priority = clip(pow(error + eps, -power), min, max)`\n",
    "                    fErrorPower = 0.6\n",
    "                    fMinPriority = 0.1\n",
    "                    fMaxPriority = 1.0\n",
    "\n",
    "                    tfReplayErrorPower = tfConstant('fErrorPower', tfFloat, fErrorPower)\n",
    "                    tfReplayMinPriority = tfConstant('fMinPriority', tfFloat, fMinPriority)\n",
    "                    tfReplayMaxPriority = tfConstant('fMaxPriority', tfFloat, fMaxPriority)\n",
    "                    tafReplayMaxPriority = tfConstant('afMaxPriority', (tfFloat, [1]), [fMaxPriority])\n",
    "                    tfReplayPriorityEpsilon = tfConstant('fPriorityEpsilon', tfFloat, 0.01)\n",
    "\n",
    "                    # Coefficients to calculate weights based on priorities\n",
    "                    fWeightPower = 0.4\n",
    "                    uWeightFeedSteps = 2e5\n",
    "\n",
    "                    tfReplayWeightDiff = tfConstant('fWeightDiff', tfFloat, (1.0 - fWeightPower) / float(uWeightFeedSteps))\n",
    "\n",
    "            with tf1.variable_scope('Var', reuse=tf1.AUTO_REUSE):\n",
    "                # Cyclic buffer: observations, actions, rewards\n",
    "                self.tafReplayPrevObservations = tfGlobalVariable('afPrevObservations', (tfFloat, [uReplayCapacity, self.oPlayer.oEnvironment.uObservationSize]), 0)\n",
    "                aReplayBufferVariables.append(self.tafReplayPrevObservations)\n",
    "                self.tafReplayObservations = tfGlobalVariable('afObservations', (tfFloat, [uReplayCapacity, self.oPlayer.oEnvironment.uObservationSize]), 0)\n",
    "                aReplayBufferVariables.append(self.tafReplayObservations)\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    self.tauReplayActions = tfGlobalVariable('auActions', (tfInt, [uReplayCapacity]), 0)\n",
    "                    aReplayBufferVariables.append(self.tauReplayActions)\n",
    "                else:\n",
    "                    self.tafReplayActions = tfGlobalVariable('afActions', (tfFloat, [uReplayCapacity, self.oPlayer.oEnvironment.uActionsSize]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayActions)\n",
    "                if self.bEpisodeRating:\n",
    "                    self.tafReplayRatings = tfGlobalVariable('afRatings', (tfFloat, [uReplayCapacity]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayRatings)\n",
    "                else:\n",
    "                    self.tafReplayRewards = tfGlobalVariable('afRewards', (tfFloat, [uReplayCapacity]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayRewards)\n",
    "                self.tafReplayDones = tfGlobalVariable('afDones', (tfFloat, [uReplayCapacity]), 0)\n",
    "                aReplayBufferVariables.append(self.tafReplayDones)\n",
    "\n",
    "                # Tree-based buffer for quick search and random sampling based on priority\n",
    "                if self.bPriorityMode:\n",
    "                    # Power of priority significance (tends to 1 over time)\n",
    "                    self.tfReplayWeightPower = tfGlobalVariable('fWeightPower', tfFloat, fWeightPower)\n",
    "                    aReplayBufferVariables.append(self.tfReplayWeightPower)\n",
    "\n",
    "                    # Tree-based buffer of maximums\n",
    "                    self.tafReplayMaxTree = tfGlobalVariable('afMaxTree', (tfFloat, [uTreeSize * 2]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplayMaxTree)\n",
    "                    # Tree-based buffer of sums\n",
    "                    self.tafReplaySumTree = tfGlobalVariable('afSumTree', (tfFloat, [uTreeSize * 2]), 0)\n",
    "                    aReplayBufferVariables.append(self.tafReplaySumTree)\n",
    "\n",
    "            # The `fnAdd` function block that adds entries to the retry buffer\n",
    "            with tf.name_scope('fnAdd'):\n",
    "                # Input data\n",
    "                with tf1.variable_scope('Input', reuse=tf1.AUTO_REUSE):\n",
    "                    self.tinafReplayPrevObservation = tfInput('afPrevObservation', (tfFloat, [self.oPlayer.oEnvironment.uObservationSize]))\n",
    "                    self.tinafReplayObservation = tfInput('afObservation', (tfFloat, [self.oPlayer.oEnvironment.uObservationSize]))\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        self.tinauReplayAction = tfInput('auAction', (tfInt, [1]))\n",
    "                    else:\n",
    "                        self.tinafReplayActions = tfInput('afActions', (tfFloat, [self.oPlayer.oEnvironment.uActionsSize]))\n",
    "                    if self.bEpisodeRating:\n",
    "                        self.tinafReplayRating = tfInput('afRating', (tfFloat, [1]))\n",
    "                    else:\n",
    "                        self.tinafReplayReward = tfInput('afReward', (tfFloat, [1]))\n",
    "                    self.tinfReplayDone = tfInput('fDone', tfFloat)\n",
    "                    self.tinfReplayScore = tfInput('fScore', tfFloat)\n",
    "                    self.tinuReplaySteps = tfInput('uSteps', tfInt)\n",
    "\n",
    "                tafDones = tf.expand_dims(self.tinfReplayDone, axis=-1)\n",
    "\n",
    "                # Index limit\n",
    "                tuMaxIndex = self.tuReplayStart + self.tuReplayCapacity\n",
    "\n",
    "                # Save input to buffer\n",
    "                tuIndex = tf.math.floormod(self.tuReplayEnd, self.tuReplayCapacity)\n",
    "                tauIndices = tf.expand_dims(tf.expand_dims(tuIndex, axis=-1), axis=-1)\n",
    "                topUpdate1 = self.tafReplayPrevObservations.scatter_nd_update(tauIndices, self.tinafReplayPrevObservation[None, :])\n",
    "                topUpdate2 = self.tafReplayObservations.scatter_nd_update(tauIndices, self.tinafReplayObservation[None, :])\n",
    "                if oPlayer.oEnvironment.bDiscrete:\n",
    "                    topUpdate3 = self.tauReplayActions.scatter_nd_update(tauIndices, self.tinauReplayAction)\n",
    "                else:\n",
    "                    topUpdate3 = self.tafReplayActions.scatter_nd_update(tauIndices, self.tinafReplayActions[None, :])\n",
    "                if self.bEpisodeRating:\n",
    "                    topUpdate4 = self.tafReplayRatings.scatter_nd_update(tauIndices, self.tinafReplayRating)\n",
    "                else:\n",
    "                    topUpdate4 = self.tafReplayRewards.scatter_nd_update(tauIndices, self.tinafReplayReward)\n",
    "                topUpdate5 = self.tafReplayDones.scatter_nd_update(tauIndices, tafDones)\n",
    "\n",
    "                with tfWait([topUpdate1, topUpdate2, topUpdate3, topUpdate4, topUpdate5]):\n",
    "                    tuNewEnd = self.tuReplayEnd.assign_add(tuOne)\n",
    "\n",
    "                with tfWait([tuNewEnd]):\n",
    "                    # Check buffer overflow\n",
    "                    def fnOverflow():\n",
    "                        return self.tuReplayStart.assign(self.tuReplayEnd + tuOne - self.tuReplayCapacity)\n",
    "                    def fnNoOverflow():\n",
    "                        return self.tuReplayStart\n",
    "                    tuNewStart = tf.cond(tf.greater(self.tuReplayEnd, tuMaxIndex), fnOverflow, fnNoOverflow, 'uNewStart')\n",
    "\n",
    "                aWaitList = [tuNewStart]\n",
    "\n",
    "                # Log episode total score\n",
    "                if self.uLogLevel > 0:\n",
    "                    def fnAddLogEpisode():\n",
    "                        with self.oSummaryWriter.as_default(): # pylint: disable=not-context-manager\n",
    "                            topLogScore = tf.summary.scalar('Stats/Score', self.tinfReplayScore, tf.cast(self.tuEpisode, tf.int64))\n",
    "                            if self.uLogLevel > 1:\n",
    "                                topLogSteps = tf.summary.scalar('Info/Steps', self.tinuReplaySteps, tf.cast(self.tuEpisode, tf.int64))\n",
    "                                topLog = tf.group(topLogScore, topLogSteps)\n",
    "                            else:\n",
    "                                topLog = topLogScore\n",
    "                        return topLog\n",
    "                    def fnAddLogStep():\n",
    "                        return tf.no_op()\n",
    "                    aWaitList.append(tf.cond(tf.equal(self.tinfReplayDone, tfZero), true_fn=fnAddLogStep, false_fn=fnAddLogEpisode))\n",
    "\n",
    "                with tfWait(aWaitList):\n",
    "                    # Go to the next episode\n",
    "                    def fnNextEpisode():\n",
    "                        return self.tuEpisode.assign_add(tuOne)\n",
    "                    def fnCurEpisode():\n",
    "                        return self.tuEpisode\n",
    "                    tuNewEpisode = tf.cond(tf.not_equal(self.tinfReplayDone, tfZero), fnNextEpisode, fnCurEpisode, 'uNewEpisode')\n",
    "\n",
    "                # Update tree-based buffer priorities\n",
    "                if self.bPriorityMode:\n",
    "                    tuIndex = tuIndex + tuReplayTreeSize\n",
    "                    tauIndices = tf.expand_dims(tf.expand_dims(tuIndex, axis=-1), axis=-1)\n",
    "                    topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauIndices, tafReplayMaxPriority)\n",
    "                    topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauIndices, tafReplayMaxPriority)\n",
    "                    tuIndex = tf.math.floordiv(tuIndex, tuTwo)\n",
    "                    with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                        def fnAddCompare(tuLoopIndex):\n",
    "                            return tf.greater_equal(tuLoopIndex, tuOne)\n",
    "                        def fnAddLoop(tuLoopIndex):\n",
    "                            tauLeft = tf.expand_dims(tuLoopIndex * tuTwo, axis=-1)\n",
    "                            tauRight = tauLeft + tuOne\n",
    "                            tfMax = tf.maximum(tf.gather(self.tafReplayMaxTree, tauLeft), tf.gather(self.tafReplayMaxTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tfSum = tf.add(tf.gather(self.tafReplaySumTree, tauLeft), tf.gather(self.tafReplaySumTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tauIndices = tf.expand_dims(tf.expand_dims(tuLoopIndex, axis=-1), axis=-1)\n",
    "                            topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauIndices, tfMax)\n",
    "                            topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauIndices, tfSum)\n",
    "                            with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                                tuLoopIndex = tf.math.floordiv(tuLoopIndex, tuTwo)\n",
    "                                return [tuLoopIndex]\n",
    "                        [topUpdateTree] = tf.while_loop(fnAddCompare, fnAddLoop, [tuIndex])\n",
    "\n",
    "            # The node of result of the function after adding data to the replay buffer\n",
    "            if self.bPriorityMode:\n",
    "                self.tfnReplayAdd = tf.group(tuNewEpisode, topUpdateTree)\n",
    "            else:\n",
    "                self.tfnReplayAdd = tuNewEpisode\n",
    "\n",
    "            # The `fnReadBatch` function block that reads batch of entries from the retry buffer\n",
    "            with tf.name_scope('fnReadBatch'):\n",
    "                # Creating batch indices for read from the replay buffer\n",
    "                if self.bPriorityMode:\n",
    "                    # The total amount of all priorities\n",
    "                    tfTotalPriority = tf.squeeze(tf.gather(self.tafReplaySumTree, tauOne)) # pylint: disable=no-value-for-parameter\n",
    "                    # Set of random priorities offsets\n",
    "                    tafRandomPriorities = tf.random.uniform([uBatchSize], minval=tfZero, maxval=tfTotalPriority, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                    # Search for matching indices of priorities offsets\n",
    "                    tuIndex = tuOne\n",
    "                    tauIndices = tf.ones([uBatchSize], dtype=tfInt)\n",
    "                    def fnRandomCompare(tuLoopIndex, tauIndices, tafRandomPriorities):\n",
    "                        return tf.less(tuLoopIndex, tuReplayTreeSize)\n",
    "                    def fnRandomLoop(tuLoopIndex, tauIndices, tafRandomPriorities):\n",
    "                        tauLeft = tauIndices * tuTwo\n",
    "                        tafValues = tf.gather(self.tafReplaySumTree, tauLeft) # pylint: disable=no-value-for-parameter\n",
    "                        tabLessEqual = tf.less_equal(tafValues, tafRandomPriorities)\n",
    "                        return [tuLoopIndex * tuTwo, tauLeft + tf.cast(tabLessEqual, tfInt), tafRandomPriorities - tafValues * tf.cast(tabLessEqual, tfFloat)]\n",
    "                    [tuReturnIndex, tauReplayIndices, tafReturnRandomPriorities] = tf.while_loop(fnRandomCompare, fnRandomLoop, [tuIndex, tauIndices, tafRandomPriorities])\n",
    "                    # Calculate weights for each index based on its priority\n",
    "                    tfMaxPriorityScale = tfOne / tf.squeeze(tf.gather(self.tafReplayMaxTree, tauOne)) # pylint: disable=no-value-for-parameter\n",
    "                    tafBatchPriorities = tf.gather(self.tafReplaySumTree, tauReplayIndices) # pylint: disable=no-value-for-parameter\n",
    "                    tafBatchWeights = tf.pow(tafBatchPriorities * tfMaxPriorityScale, self.tfReplayWeightPower) # pylint: disable=invalid-unary-operand-type\n",
    "                    # Update coefficient affecting weights\n",
    "                    topUpdateReplayWeightPower = self.tfReplayWeightPower.assign(tf.minimum(tfOne, self.tfReplayWeightPower + tfReplayWeightDiff))\n",
    "\n",
    "                    with tfWait([tuReturnIndex, tauReplayIndices, tafReturnRandomPriorities, topUpdateReplayWeightPower]):\n",
    "                        # Set of random indices\n",
    "                        tauRandomIndices = tauReplayIndices - tuReplayTreeSize\n",
    "                else:\n",
    "                    # Set of random offsets\n",
    "                    tauRandomOffsets = tf.random.uniform([uBatchSize], minval=self.tuReplayStart, maxval=self.tuReplayEnd, dtype=tfInt, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                    # Set of random indices\n",
    "                    tauRandomIndices = tf.math.floormod(tauRandomOffsets, self.tuReplayCapacity)\n",
    "\n",
    "                # Create data block based on a sample of random indices\n",
    "                tafBatchPrevObservations = tf.gather(self.tafReplayPrevObservations, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                tafBatchObservations = tf.gather(self.tafReplayObservations, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tauBatchActions = tf.gather(self.tauReplayActions, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                else:\n",
    "                    tafBatchActions = tf.gather(self.tafReplayActions, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                if self.bEpisodeRating:\n",
    "                    tafBatchRatings = tf.gather(self.tafReplayRatings, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                else:\n",
    "                    tafBatchRewards = tf.gather(self.tafReplayRewards, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "                tafBatchDones = tf.gather(self.tafReplayDones, tauRandomIndices) # pylint: disable=no-value-for-parameter\n",
    "\n",
    "        # Body `Soft Actor Critic` agent\n",
    "\n",
    "        with tf.name_scope('SacAgent'):\n",
    "            with tf.name_scope('Critic'):\n",
    "                # Create two neural networks ``Trainer Critic` and two neural networks `Target Critic`\n",
    "                self.annCriticNetworks = [CriticNetwork() for _ in range(4)]\n",
    "                self.nnTrainCritic1 = self.annCriticNetworks[0]\n",
    "                self.nnTrainCritic2 = self.annCriticNetworks[1]\n",
    "                self.nnTargetCritic1 = self.annCriticNetworks[2]\n",
    "                self.nnTargetCritic2 = self.annCriticNetworks[3]\n",
    "                [nnCritic.build((self.uBatchSize, oEnvironment.uObservationSize), (self.uBatchSize, oEnvironment.uActionsSize)) for nnCritic in self.annCriticNetworks]\n",
    "                [aTrainVariables.extend(nnCritic.trainable_variables) for nnCritic in self.annCriticNetworks]\n",
    "\n",
    "            with tf.name_scope('Actor'):\n",
    "                # Create neural network `Trainer Actor`\n",
    "                self.nnTrainActor = CreateActor(oEnvironment, self.uBatchSize, 'nnSacTrainActor')\n",
    "                aTrainVariables.extend(self.nnTrainActor.trainable_variables)\n",
    "\n",
    "                if not self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    afActionsMin = np.array([0])\n",
    "                    afActionsMax = np.array([1])\n",
    "\n",
    "                    # Constants for continous model\n",
    "                    with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "                        tafActionsMean = tfConstant('afActionsMean', tfFloat, (afActionsMin + afActionsMax) / 2)\n",
    "                        tafActionsStd = tfConstant('afActionsStd', tfFloat, (afActionsMin - afActionsMax) / 2)\n",
    "\n",
    "            with tf1.variable_scope('Var', reuse=tf1.AUTO_REUSE):\n",
    "                # `Alpha Regulator`\n",
    "                self.tfLogAlpha = tfGlobalVariable('fLogAlpha', tfFloat, 0, True)\n",
    "                aTrainVariables.append(self.tfLogAlpha)\n",
    "\n",
    "                if self.uLogLevel > 1:\n",
    "                    tfCriticGradientAverageNormal = tfGlobalVariable('fCriticGradientClip', tfFloat, 0)\n",
    "                    aTrainVariables.append(tfCriticGradientAverageNormal)\n",
    "                    tfActorGradientAverageNormal = tfGlobalVariable('fActorGradientClip', tfFloat, 0)\n",
    "                    aTrainVariables.append(tfActorGradientAverageNormal)\n",
    "\n",
    "                if tfTrainStepCounter is None:\n",
    "                    tfTrainStepCounter = tf.compat.v1.train.get_or_create_global_step()\n",
    "                aTrainVariables.append(tfTrainStepCounter)\n",
    "\n",
    "            with tf1.variable_scope('Const', reuse=tf1.AUTO_REUSE):\n",
    "                # Factor of discounting rating\n",
    "                tfRatingDiscountFactor = tfConstant('fRatingDiscountFactor', tfFloat, self.fRatingDiscountFactor)\n",
    "                # Target entropy\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tfTargetEntropy = tfConstant('fTargetEntropy', tfFloat, -np.log(1.0 / oEnvironment.uActionsSize) * 0.98)\n",
    "                else:\n",
    "                    tfTargetEntropy = tfConstant('fTargetEntropy', tfFloat, oEnvironment.uActionsSize / 2.0)\n",
    "\n",
    "            # The `fnInitialize` function block that initialize all variables on first start\n",
    "            with tf.name_scope('fnInitialize'):\n",
    "                topCriticUpdate1 = fnHardUpdate(self.nnTargetCritic1.variables, self.nnTrainCritic1.variables)\n",
    "                topCriticUpdate2 = fnHardUpdate(self.nnTargetCritic2.variables, self.nnTrainCritic2.variables)\n",
    "                topActorUpdate = fnHardUpdate(self.oPlayer.nnActor.variables, self.nnTrainActor.variables)\n",
    "            # The node of result of the function after initialization\n",
    "            self.tfnInitialize = tf.group(topCriticUpdate1, topCriticUpdate2, topActorUpdate)\n",
    "\n",
    "            # Update target neural networks\n",
    "            def fnUpdateTarget():\n",
    "                topCriticUpdate1 = fnSoftUpdate(self.nnTargetCritic1.variables, self.nnTrainCritic1.variables, tfZero, tfOne, tfTrainableTargetAverageForgetCoef, tfTrainableTargetAverageUpdateCoef)\n",
    "                topCriticUpdate2 = fnSoftUpdate(self.nnTargetCritic2.variables, self.nnTrainCritic2.variables, tfZero, tfOne, tfTrainableTargetAverageForgetCoef, tfTrainableTargetAverageUpdateCoef)\n",
    "                topActorUpdate = fnHardUpdate(self.oPlayer.nnActor.variables, self.nnTrainActor.variables)\n",
    "                return tf.group(topCriticUpdate1, topCriticUpdate2, topActorUpdate)\n",
    "\n",
    "            # The `fnTrain` function block that performs all operations related to training networks\n",
    "            with tf.name_scope('fnTrain'):\n",
    "                # Format input data\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tafActionsOneHots = tf.one_hot(tauBatchActions, tuActionsSize, on_value=tfOne, off_value=tfZero, dtype=tfFloat) # pylint: disable=unexpected-keyword-arg\n",
    "                    tafStates = self.nnTrainCritic1.prepare(tafBatchPrevObservations, tafActionsOneHots)\n",
    "                else:\n",
    "                    tafStates = self.nnTrainCritic1.prepare(tafBatchPrevObservations, tafBatchActions)\n",
    "\n",
    "                # Calculate ratings for input data\n",
    "                if self.bEpisodeRating:\n",
    "                    # For episode mode, ratings are already calculated in advance\n",
    "                    tafRatings = tafBatchRatings\n",
    "                else:\n",
    "                    # Calculate the probabilities of the next actions\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        # Unnormalized predicted logarithmic probabilities of possible actions\n",
    "                        tafNextUnnormalizedLogProbabilities = self.nnTrainActor.call(tafBatchObservations)\n",
    "                        # Array of random numbers to add noise to the logarithmic probabilities\n",
    "                        tafRandomUniforms = tf.random.uniform([self.uBatchSize, oEnvironment.uActionsSize], minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                        # Gumbel distribution for random numbers\n",
    "                        tafGumbels = -tf.math.log(-tf.math.log(tafRandomUniforms)) # pylint: disable=invalid-unary-operand-type\n",
    "                        # Unnormalized predicted logarithmic probabilities of possible actions with added noise\n",
    "                        tafNextUnnormalizedNoisyLogProbabilities = tafNextUnnormalizedLogProbabilities + tafGumbels\n",
    "                        # Best estimated actions\n",
    "                        tauNextBestActions = tf.argmax(tafNextUnnormalizedNoisyLogProbabilities, axis=-1, output_type=tfInt)\n",
    "                        # Best predicted actions vectors\n",
    "                        tafNextPredActions = tf.one_hot(tauNextBestActions, tuActionsSize, on_value=tfOne, off_value=tfZero, dtype=tfFloat) # pylint: disable=unexpected-keyword-arg\n",
    "                        # Normalized predicted logarithmic probabilities of possible actions\n",
    "                        tafNextNormalizedLogProbabilities = tf.math.log_softmax(tafNextUnnormalizedLogProbabilities, axis=-1)\n",
    "                        # Logarithmic probabilities of the best predicted actions (in the role of entropy)\n",
    "                        tafNextProbabilitiesEntropy = -tf.math.reduce_sum(tafNextPredActions * tafNextNormalizedLogProbabilities, axis=-1)\n",
    "                    else:\n",
    "                        tafNextLocations, tafNextScales = self.nnTrainActor.call(tafBatchObservations)\n",
    "                        tafNextClippedScales = tf.clip_by_value(tafNextScales, tfTotalMinScale, tfTotalMaxScale)\n",
    "                        tafNextClippedScalesExp = tf.math.exp(tafNextClippedScales)\t\t\t\n",
    "                        tafNextRandomNormals = tf.random.normal([self.uBatchSize, oEnvironment.uActionsSize], mean=tafNextLocations, stddev=tafNextClippedScalesExp, dtype=tfFloat, seed=None)\n",
    "                        tafNextPredActions = tf.math.tanh(tafNextRandomNormals) * tafActionsStd + tafActionsMean\n",
    "                        tafNextClippedScalesExpSquare = tf.square(tafNextClippedScalesExp)\n",
    "                        tafNextProbabilitiesEntropy = (tf.square(tafNextPredActions - tafNextLocations)) / (2 * tafNextClippedScalesExpSquare) + tafNextClippedScales + tfLogSqrtPi2 # pylint: disable=invalid-unary-operand-type\n",
    "\n",
    "                    # Calculate the following probable ratings\n",
    "                    tafNextStates = self.nnTargetCritic1.prepare(tafBatchObservations, tafNextPredActions)\n",
    "                    tafPredNextRatings1 = tf.squeeze(self.nnTargetCritic1.call(tafNextStates), axis=-1)\n",
    "                    tafPredNextRatings2 = tf.squeeze(self.nnTargetCritic2.call(tafNextStates), axis=-1)\n",
    "                    tafPredNextRatings = tf.minimum(tafPredNextRatings1, tafPredNextRatings2) + tf.math.exp(self.tfLogAlpha) * tafNextProbabilitiesEntropy\n",
    "\n",
    "                    # Calculate ratings\n",
    "                    tafRatings = tafBatchRewards + (1. - tafBatchDones) * tafPredNextRatings * tfRatingDiscountFactor\n",
    "\n",
    "                # Calculate the error of learning the neural network `Trainer Critic` #1\n",
    "                tfaTrainableCriticVariables1 = self.nnTrainCritic1.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableCriticVariables1)\n",
    "\n",
    "                    # Predicted ratings\n",
    "                    tafPredRatings1 = tf.squeeze(self.nnTrainCritic1.call(tf.stop_gradient(tafStates)), axis=-1)\n",
    "                    # The advantage of real ratings over predicted ones\n",
    "                    tafAdvantages1 = tf.stop_gradient(tafRatings) - tafPredRatings1\n",
    "                    # Learning error\n",
    "                    if self.bPriorityMode:\n",
    "                        tfCriticLoss1 = tf.reduce_mean(tf.square(tafAdvantages1) * tf.stop_gradient(tafBatchWeights))\n",
    "                    else:\n",
    "                        tfCriticLoss1 = tf.reduce_mean(tf.square(tafAdvantages1))\n",
    "\n",
    "                # Calculate gradients\n",
    "                aCriticGradients1 = tape.gradient(tfCriticLoss1, tfaTrainableCriticVariables1)\n",
    "\n",
    "                # Calculate the error of learning the neural network `Trainer Critic` #2\n",
    "                tfaTrainableCriticVariables2 = self.nnTrainCritic2.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableCriticVariables2)\n",
    "\n",
    "                    # Predicted ratings\n",
    "                    tafPredRatings2 = tf.squeeze(self.nnTrainCritic2.call(tf.stop_gradient(tafStates)), axis=-1)\n",
    "                    # The advantage of real ratings over predicted ones\n",
    "                    tafAdvantages2 = tf.stop_gradient(tafRatings) - tafPredRatings2\n",
    "                    # Learning error\n",
    "                    if self.bPriorityMode:\n",
    "                        tfCriticLoss2 = tf.reduce_mean(tf.square(tafAdvantages2) * tf.stop_gradient(tafBatchWeights))\n",
    "                    else:\n",
    "                        tfCriticLoss2 = tf.reduce_mean(tf.square(tafAdvantages2))\n",
    "\n",
    "                # Calculate gradients\n",
    "                aCriticGradients2 = tape.gradient(tfCriticLoss2, tfaTrainableCriticVariables2)\n",
    "\n",
    "                # Total critic error\n",
    "                tfCriticLoss = (tfCriticLoss1 + tfCriticLoss2) * tfHalf\n",
    "\n",
    "                # Calculate the average normal value of the `Trainer Critic` gradients\n",
    "                if self.uLogLevel > 1:\n",
    "                    tuGradientsCount = tfZero\n",
    "                    tfGradientsNormalMass = tfZero\n",
    "                    for tfaGradientsGroup in aCriticGradients1:\n",
    "                        tfaGradients = tfaGradientsGroup.values if isinstance(tfaGradientsGroup, tf.IndexedSlices) else tfaGradientsGroup\n",
    "                        tuCount = tf.cast(tf.size(tfaGradients), dtype=tfFloat) # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n",
    "                        tuGradientsCount += tuCount\n",
    "                        tfGradientsNormalMass += tf.linalg.global_norm([tfaGradients]) * tuCount\n",
    "                    for tfaGradientsGroup in aCriticGradients2:\n",
    "                        tfaGradients = tfaGradientsGroup.values if isinstance(tfaGradientsGroup, tf.IndexedSlices) else tfaGradientsGroup\n",
    "                        tuCount = tf.cast(tf.size(tfaGradients), dtype=tfFloat) # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n",
    "                        tuGradientsCount += tuCount\n",
    "                        tfGradientsNormalMass += tf.linalg.global_norm([tfaGradients]) * tuCount\n",
    "                    tfCriticGradientNormal = tfCriticGradientAverageNormal.assign(tfCriticGradientAverageNormal * tfGradientNormalForgetCoef + (tfGradientsNormalMass / tuGradientsCount) * tfGradientNormalUpdateCoef)\n",
    "\n",
    "                # Trim gradients values to prevent `inf` and` nan` errors\n",
    "                if fMaxGradientNormal is not None:\n",
    "                    aCriticGradients1 = fnClipGradients(aCriticGradients1, tfMaxGradientNormal)\n",
    "                    aCriticGradients2 = fnClipGradients(aCriticGradients2, tfMaxGradientNormal)\n",
    "\n",
    "                # Update priorities in replay buffer\n",
    "                if self.bPriorityMode:\n",
    "                    # Calculate priorities based on adantages of ratings\n",
    "                    tafReplayErrors = tf.minimum(tf.abs(tafAdvantages1), tf.abs(tafAdvantages2))\n",
    "                    tafReplayPriorities = tf.clip_by_value(tf.pow(tafReplayErrors + tfReplayPriorityEpsilon, -tfReplayErrorPower), tfReplayMinPriority, tfReplayMaxPriority)\n",
    "                    # Update priorities\n",
    "                    tauUpdateIndices = tf.expand_dims(tauReplayIndices, axis=-1)\n",
    "                    topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauUpdateIndices, tafReplayPriorities)\n",
    "                    topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauUpdateIndices, tafReplayPriorities)\n",
    "                    with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                        tuIndex = tuHalfReplayTreeSize\n",
    "                        tauIndices = tf.math.floordiv(tauReplayIndices, tuTwo)\n",
    "                        def fnUpdateCompare(tuLoopIndex, tauIndices):\n",
    "                            return tf.greater_equal(tuLoopIndex, tuOne)\n",
    "                        def fnUpdateLoop(tuLoopIndex, tauIndices):\n",
    "                            tauLeft = tauIndices * tuTwo\n",
    "                            tauRight = tauLeft + tuOne\n",
    "                            tfMax = tf.maximum(tf.gather(self.tafReplayMaxTree, tauLeft), tf.gather(self.tafReplayMaxTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tfSum = tf.add(tf.gather(self.tafReplaySumTree, tauLeft), tf.gather(self.tafReplaySumTree, tauRight)) # pylint: disable=no-value-for-parameter\n",
    "                            tauUpdateIndices = tf.expand_dims(tauIndices, axis=-1)\n",
    "                            topUpdateMax = self.tafReplayMaxTree.scatter_nd_update(tauUpdateIndices, tfMax)\n",
    "                            topUpdateSum = self.tafReplaySumTree.scatter_nd_update(tauUpdateIndices, tfSum)\n",
    "                            with tfWait([topUpdateMax, topUpdateSum]):\n",
    "                                tauIndices = tf.math.floordiv(tauIndices, tuTwo)\n",
    "                                tuLoopIndex = tf.math.floordiv(tuLoopIndex, tuTwo)\n",
    "                                return [tuLoopIndex, tauIndices]\n",
    "                        [tuIndex, tauIndices] = tf.while_loop(fnUpdateCompare, fnUpdateLoop, [tuIndex, tauIndices])\n",
    "                        topUpdateReplayPriorities = tf.group(tuIndex, tauIndices)\n",
    "\n",
    "                # Calculate the error of learning the neural network `Trainer Actor`\n",
    "                tfaTrainableActorVariables = self.nnTrainActor.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableActorVariables)\n",
    "\n",
    "                    # Calculate the probabilities of all possible actions\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        # Unnormalized predicted logarithmic probabilities of actions\n",
    "                        tafUnnormalizedLogProbabilities = self.nnTrainActor.call(tf.stop_gradient(tafBatchPrevObservations))\n",
    "                        # Array of random numbers to add noise to the logarithmic probabilities\n",
    "                        tafRandomUniforms = tf.random.uniform([self.uBatchSize, oEnvironment.uActionsSize], minval=tfTotalMinLogInput, maxval=tfTotalMaxLogInput, dtype=tfFloat, seed=None) # pylint: disable=unexpected-keyword-arg\n",
    "                        # Gumbel distribution for random numbers\n",
    "                        tafGumbels = -tf.math.log(-tf.math.log(tafRandomUniforms)) # pylint: disable=invalid-unary-operand-type\n",
    "                        # Unnormalized predicted logarithmic probabilities of possible actions with added noise\n",
    "                        tafUnnormalizedNoisyLogProbabilities = tafUnnormalizedLogProbabilities + tafGumbels\n",
    "                        # Predicted actions vectors\n",
    "                        tafPredActions = tf.math.exp(tafUnnormalizedNoisyLogProbabilities - tf.math.reduce_logsumexp(tafUnnormalizedNoisyLogProbabilities, axis=-1, keepdims=True))\n",
    "                        # Normalized predicted logarithmic probabilities of possible actions\n",
    "                        tafPredNormalizedLogProbabilities = tf.math.log_softmax(tafUnnormalizedLogProbabilities, axis=-1)\n",
    "                        # Logarithmic probabilities of the best predicted actions (in the role of entropy)\n",
    "                        tafPredProbabilitiesEntropy = -tf.math.reduce_sum(tafPredActions * tafPredNormalizedLogProbabilities, axis=-1)\n",
    "                    else:\n",
    "                        tafLocations, tafScales = self.nnTrainActor.call(tf.stop_gradient(tafBatchPrevObservations))\n",
    "                        tafClippedScales = tf.clip_by_value(tafScales, tfTotalMinScale, tfTotalMaxScale)\n",
    "                        tafClippedScalesExp = tf.math.exp(tafClippedScales)\n",
    "                        tafRandomNormals = tf.random.normal([self.uBatchSize,oEnvironment.uActionsSize], mean=tfZero, stddev=tfOne, dtype=tfFloat, seed=None)\n",
    "                        tafRandomUnscaledActions = tafLocations + tafClippedScalesExp * tafRandomNormals\n",
    "                        tafPredActions = tf.math.tanh(tafRandomUnscaledActions) * tafActionsStd + tafActionsMean\n",
    "                        tafClippedScalesExpPower = tf.math.square(tafClippedScalesExp)\n",
    "                        tafNormalizers = -tf.math.reduce_sum(tf.math.log(1 - tf.math.square(tafPredActions) + 1e-6), axis=1)\n",
    "                        tafPredProbabilitiesEntropy = (tf.math.square(tafRandomUnscaledActions - tafLocations)) / (2 * tafClippedScalesExpPower) + tafClippedScales + tfLogSqrtPi2 + tafNormalizers # pylint: disable=invalid-unary-operand-type\n",
    "\n",
    "                    # Calculate the predicted probable ratings\n",
    "                    tafRandomStates = self.nnTrainCritic1.prepare(tf.stop_gradient(tafBatchPrevObservations), tafPredActions)\n",
    "                    tafRandomPredRatings1 = tf.squeeze(self.nnTrainCritic1.call(tafRandomStates), axis=-1)\n",
    "                    tafRandomPredRatings2 = tf.squeeze(self.nnTrainCritic2.call(tafRandomStates), axis=-1)\n",
    "                    tafRandomPredRatings = tf.minimum(tafRandomPredRatings1, tafRandomPredRatings2) + tf.exp(self.tfLogAlpha) * tafPredProbabilitiesEntropy\n",
    "\n",
    "                    # Learning error\n",
    "                    if self.bPriorityMode:\n",
    "                        tfActorLoss = -tf.math.reduce_mean(tafRandomPredRatings * tf.stop_gradient(tafBatchWeights))\n",
    "                    else:\n",
    "                        tfActorLoss = -tf.math.reduce_mean(tafRandomPredRatings)\n",
    "\n",
    "                # Calculate gradients\n",
    "                aActorGradients = tape.gradient(tfActorLoss, tfaTrainableActorVariables)\n",
    "\n",
    "                # Calculate the average normal value of the `Trainer Actor` gradients\n",
    "                if self.uLogLevel > 1:\n",
    "                    tuGradientsCount = tfZero\n",
    "                    tfGradientsNormalMass = tfZero\n",
    "                    for tfaGradientsGroup in aActorGradients:\n",
    "                        tfaGradients = tfaGradientsGroup.values if isinstance(tfaGradientsGroup, tf.IndexedSlices) else tfaGradientsGroup\n",
    "                        tuCount = tf.cast(tf.size(tfaGradients), dtype=tfFloat) # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n",
    "                        tuGradientsCount += tuCount\n",
    "                        tfGradientsNormalMass += tf.linalg.global_norm([tfaGradients]) * tuCount\n",
    "                    tfActorGradientNormal = tfActorGradientAverageNormal.assign(tfActorGradientAverageNormal * tfGradientNormalForgetCoef + (tfGradientsNormalMass / tuGradientsCount) * tfGradientNormalUpdateCoef)\n",
    "\n",
    "                # Trim gradients values to prevent `inf` and` nan` errors\n",
    "                if fMaxGradientNormal is not None:\n",
    "                    aActorGradients = fnClipGradients(aActorGradients, tfMaxGradientNormal)\n",
    "\n",
    "                # Calculate the error of learning the `Alpha Regulator`\n",
    "                tfaTrainableAlphaVariable = [self.tfLogAlpha]\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(tfaTrainableAlphaVariable)\n",
    "\n",
    "                    # Calculate entropy loss\n",
    "                    tafEntropyLoss = -(tafPredProbabilitiesEntropy - tfTargetEntropy)\n",
    "\n",
    "                    # Learning error\n",
    "                    if self.bPriorityMode:\n",
    "                        tfAlphaLoss = tf.math.reduce_mean(tafEntropyLoss * tf.stop_gradient(tafBatchWeights)) * self.tfLogAlpha\n",
    "                    else:\n",
    "                        tfAlphaLoss = tf.math.reduce_mean(tafEntropyLoss) * self.tfLogAlpha\n",
    "\n",
    "                # Calculate gradients\n",
    "                aAlphaGradient = tape.gradient(tfAlphaLoss, tfaTrainableAlphaVariable)\n",
    "\n",
    "                # Trim gradients values to prevent `inf` and` nan` errors\n",
    "                if fMaxGradientNormal is not None:\n",
    "                    aAlphaGradient = fnClipGradients(aAlphaGradient, tfMaxGradientNormal)\n",
    "\n",
    "                # Training\n",
    "                with tfWait([tfCriticLoss, tfActorLoss, tfAlphaLoss]):\n",
    "                    topOptimizeCritic1 = self.koCriticOptimizer.apply_gradients(zip(aCriticGradients1, tfaTrainableCriticVariables1))\n",
    "                    topOptimizeCritic2 = self.koCriticOptimizer.apply_gradients(zip(aCriticGradients2, tfaTrainableCriticVariables2))\n",
    "                    topOptimizeActor = self.koActorOptimizer.apply_gradients(zip(aActorGradients, tfaTrainableActorVariables))\n",
    "                    topOptimizeAlpha = self.koAlphaOptimizer.apply_gradients(zip(aAlphaGradient, tfaTrainableAlphaVariable))\n",
    "\n",
    "                aWaitList = [topOptimizeCritic1, topOptimizeCritic2, topOptimizeActor, topOptimizeAlpha]\n",
    "\n",
    "                if self.bPriorityMode:\n",
    "                    aWaitList.append(topUpdateReplayPriorities)\n",
    "\n",
    "        # Logging function subblock that performs all operations related to statistics\n",
    "\n",
    "        if self.uLogLevel > 0:\n",
    "            with self.oSummaryWriter.as_default(): # pylint: disable=not-context-manager\n",
    "                aWaitList.append(tf.summary.scalar('Stats/Loss/Critic', tf.reduce_mean(tfCriticLoss), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Stats/Loss/Actor', tf.reduce_mean(tfActorLoss), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Stats/Loss/Alpha', tfAlphaLoss, tfTrainStepCounter))\n",
    "\n",
    "        if self.uLogLevel > 1:\n",
    "            with self.oSummaryWriter.as_default(): # pylint: disable=not-context-manager\n",
    "                aWaitList.append(tf.summary.scalar('Info/GradientNormal/Critic', tfCriticGradientNormal, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Info/GradientNormal/Actor', tfActorGradientNormal, tfTrainStepCounter))\n",
    "                if self.bPriorityMode:\n",
    "                    aWaitList.append(tf.summary.scalar('Info/LossWeight/Mean', tf.reduce_mean(tafBatchWeights), tfTrainStepCounter))\n",
    "\n",
    "                aWaitList.append(tf.summary.scalar('Rating/LogAlpha', self.tfLogAlpha, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Rating/Alpha', tf.math.exp(self.tfLogAlpha), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Rating/Value/Mean', tf.reduce_mean(tafRatings), tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.scalar('Rating/Entropy/Mean', tf.reduce_mean(tafPredProbabilitiesEntropy), tfTrainStepCounter))\n",
    "\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Value', tafRatings, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Entropy', tafPredProbabilitiesEntropy, tfTrainStepCounter))\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Pred', tafRandomPredRatings, tfTrainStepCounter))\n",
    "                tafAdvantages = tf.concat([tafAdvantages1, tafAdvantages2], 0)\n",
    "                aWaitList.append(tf.summary.histogram('Rating/Advantage', tafAdvantages, tfTrainStepCounter))\n",
    "\n",
    "        if self.uLogLevel > 2:\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aCriticGradients1, tfaTrainableCriticVariables1), tfTrainStepCounter))\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aCriticGradients2, tfaTrainableCriticVariables2), tfTrainStepCounter))\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aActorGradients, tfaTrainableActorVariables), tfTrainStepCounter))\n",
    "            aWaitList.append(fnWeightsSummary(self.oSummaryWriter, zip(aAlphaGradient, tfaTrainableAlphaVariable), tfTrainStepCounter))\n",
    "\n",
    "        with tf.name_scope('SacAgent'):\n",
    "            # Continue... The `fnTrain` function block that performs all operations related to training networks\n",
    "            with tf.name_scope('fnTrain'):\n",
    "                with tfWait(aWaitList):\n",
    "                    # Update step\n",
    "                    topNewStepCounter = tfTrainStepCounter.assign_add(tuOne64)\n",
    "                    # Update target networks using `moving average` method\n",
    "                    topUpdateTarget = fnUpdateTarget()\n",
    "\n",
    "                with tfWait([topNewStepCounter, topUpdateTarget]):\n",
    "                    # The node of result of the function after training\n",
    "                    self.tfnTrain = tfCriticLoss + tfActorLoss + tfAlphaLoss\n",
    "\n",
    "        # Create folders for storing the model\n",
    "\n",
    "        self.sTargetRestorePath = sRestorePath + 'Target/'\n",
    "        self.sTrainRestorePath = sRestorePath + 'Train/'\n",
    "        self.sReplayBufferRestorePath = sRestorePath + 'ReplayBuffer/'\n",
    "\n",
    "        self.oTargetSaver = tf.compat.v1.train.Saver(aTargetVariables, save_relative_paths=True)\n",
    "        self.oTrainSaver = tf.compat.v1.train.Saver(aTrainVariables, save_relative_paths=True)\n",
    "        self.oReplayBufferSaver = tf.compat.v1.train.Saver(aReplayBufferVariables, save_relative_paths=True)\n",
    "\n",
    "        if not os.path.exists(self.sTargetRestorePath):\n",
    "            os.makedirs(self.sTargetRestorePath, exist_ok=True)\n",
    "        if not os.path.exists(self.sTrainRestorePath):\n",
    "            os.makedirs(self.sTrainRestorePath, exist_ok=True)\n",
    "        if not os.path.exists(self.sReplayBufferRestorePath):\n",
    "            os.makedirs(self.sReplayBufferRestorePath, exist_ok=True)\n",
    "\n",
    "    # Internal function to fill single step\n",
    "    def __fill(self, uDebugLevel):\n",
    "        uEpisode = tfEval(self.tuEpisode)\n",
    "        bDone = self.oPlayer.next()\n",
    "        uFillCount = 0\n",
    "\n",
    "        if self.bEpisodeRating:\n",
    "            if self.uBufferSize >= self.uBufferCapacity:\n",
    "                self.uBufferCapacity += 256\n",
    "\n",
    "                self.afPrevObservations = np.resize(self.afPrevObservations, (self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize))\n",
    "                self.afObservations = np.resize(self.afObservations, (self.uBufferCapacity, self.oPlayer.oEnvironment.uObservationSize))\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    self.auActions = np.resize(self.auActions, (self.uBufferCapacity,))\n",
    "                else:\n",
    "                    self.afActions= np.resize(self.afActions, (self.uBufferCapacity, self.oPlayer.oEnvironment.uActionsSize))\n",
    "                self.afRewards = np.resize(self.afRewards, (self.uBufferCapacity,))\n",
    "                self.afRatings = np.resize(self.afRatings, (self.uBufferCapacity,))\n",
    "\n",
    "            uIndex = self.uBufferSize\n",
    "\n",
    "            self.afPrevObservations[uIndex] = self.oPlayer.afPrevObservation\n",
    "            self.afObservations[uIndex] = self.oPlayer.afObservation\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                self.auActions[uIndex] = self.oPlayer.uAction\n",
    "            else:\n",
    "                self.afActions[uIndex] = self.oPlayer.afActions\n",
    "            self.afRewards[uIndex] = self.oPlayer.fReward\n",
    "\n",
    "            self.uBufferSize += 1\n",
    "        else:\n",
    "            if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                tfEval(self.tfnReplayAdd, {\n",
    "                    self.tinafReplayPrevObservation: self.oPlayer.afPrevObservation,\n",
    "                    self.tinafReplayObservation: self.oPlayer.afObservation,\n",
    "                    self.tinauReplayAction: [self.oPlayer.uAction],\n",
    "                    self.tinafReplayReward: [self.oPlayer.fReward],\n",
    "                    self.tinfReplayDone: float(bDone),\n",
    "                    self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                    self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                })\n",
    "            else:\n",
    "                tfEval(self.tfnReplayAdd, {\n",
    "                    self.tinafReplayPrevObservation: self.oPlayer.afPrevObservation,\n",
    "                    self.tinafReplayObservation: self.oPlayer.afObservation,\n",
    "                    self.tinafReplayActions: self.oPlayer.afActions,\n",
    "                    self.tinafReplayReward: [self.oPlayer.fReward],\n",
    "                    self.tinfReplayDone: float(bDone),\n",
    "                    self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                    self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                })\n",
    "            uFillCount += 1\n",
    "\n",
    "        if bDone:\n",
    "            if self.bEpisodeRating:\n",
    "                uIndex = self.uBufferSize - 1\n",
    "                fLastScore = self.afRatings[uIndex] = self.afRewards[uIndex]\n",
    "                while uIndex > 0:\n",
    "                    uIndex -= 1\n",
    "                    fLastScore = self.afRatings[uIndex] = self.afRewards[uIndex] + self.fRatingDiscountFactor * fLastScore\n",
    "\n",
    "                for uIndex in range(self.uBufferSize - 1):\n",
    "                    if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                        tfEval(self.tfnReplayAdd, {\n",
    "                            self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                            self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                            self.tinauReplayAction: [self.auActions[uIndex]],\n",
    "                            self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                            self.tinfReplayDone: 0.0,\n",
    "                            self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                            self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                        })\n",
    "                    else:\n",
    "                        tfEval(self.tfnReplayAdd, {\n",
    "                            self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                            self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                            self.tinafReplayActions: [self.afActions[uIndex]],\n",
    "                            self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                            self.tinfReplayDone: 0.0,\n",
    "                            self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                            self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                        })\n",
    "                    uFillCount += 1\n",
    "\n",
    "                uIndex = self.uBufferSize - 1\n",
    "                if self.oPlayer.oEnvironment.bDiscrete:\n",
    "                    tfEval(self.tfnReplayAdd, {\n",
    "                        self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                        self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                        self.tinauReplayAction: [self.auActions[uIndex]],\n",
    "                        self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                        self.tinfReplayDone: 1.0,\n",
    "                        self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                        self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                    })\n",
    "                else:\n",
    "                    tfEval(self.tfnReplayAdd, {\n",
    "                        self.tinafReplayPrevObservation: self.afPrevObservations[uIndex],\n",
    "                        self.tinafReplayObservation: self.afObservations[uIndex],\n",
    "                        self.tinafReplayActions: [self.afActions[uIndex]],\n",
    "                        self.tinafReplayRating: [self.afRatings[uIndex]],\n",
    "                        self.tinfReplayDone: 1.0,\n",
    "                        self.tinfReplayScore: self.oPlayer.fScore,\n",
    "                        self.tinuReplaySteps: self.oPlayer.uStep\n",
    "                    })\n",
    "                uFillCount += 1\n",
    "\n",
    "                self.uBufferSize = 0\n",
    "\n",
    "        if uDebugLevel > 0:\n",
    "            uEnd, uStart, uReplayCapacity = tfEval([self.tuReplayEnd, self.tuReplayStart, self.tuReplayCapacity])\n",
    "\n",
    "            print('\\r[MEM:%s] Fill: Ep.%d:%d, Score %f, RewardAvg %f, Used %d of %d        ' % (\n",
    "                getMemoryUsage(),\n",
    "                uEpisode, self.oPlayer.uStep, self.oPlayer.fScore, self.oPlayer.fAverageReward,\n",
    "                uEnd - uStart,\n",
    "                uReplayCapacity\n",
    "            ), end = '\\n' if bDone else '')\n",
    "\n",
    "        if bDone:\n",
    "            self.oPlayer.reset()\n",
    "\n",
    "        return bDone, uFillCount\n",
    "\n",
    "    # Fill the repeat buffer in N steps or N episodes, depending on the setting\n",
    "    def fill(self, uFillSize=1, uDebugLevel=0, bPrefill=False):\n",
    "        if uDebugLevel > 1:\n",
    "            print('\\rFill: Processing...', end='')\n",
    "\n",
    "        uTotalFillCount = 0\n",
    "\n",
    "        for _ in range(uFillSize):\n",
    "            bDone, uFillCount = self.__fill(uDebugLevel)\n",
    "            uTotalFillCount += uFillCount\n",
    "\n",
    "        if bPrefill:\n",
    "            while not bDone:\n",
    "                bDone, uFillCount = self.__fill(uDebugLevel)\n",
    "                uTotalFillCount += uFillCount\n",
    "\n",
    "        if uDebugLevel > 1:\n",
    "            uEnd, uStart, uReplayCapacity = tfEval([self.tuReplayEnd, self.tuReplayStart, self.tuReplayCapacity])\n",
    "\n",
    "            print('\\nStatus: Used %d of %d from %d to %d' % (\n",
    "                uEnd - uStart,\n",
    "                uReplayCapacity,\n",
    "                uStart % uReplayCapacity,\n",
    "                uEnd % uReplayCapacity\n",
    "            ))\n",
    "\n",
    "        return uTotalFillCount, bDone\n",
    "\n",
    "    # Initialize graph\n",
    "    def initialize(self):\n",
    "        return tfEval([self.oSummaryWriter.init(), self.tfnInitialize])\n",
    "\n",
    "    # Save model\n",
    "    def save(self):\n",
    "        self.oTargetSaver.save(tfActiveSession.tfoSession, self.sTargetRestorePath)\n",
    "        self.oTrainSaver.save(tfActiveSession.tfoSession, self.sTrainRestorePath)\n",
    "        self.oReplayBufferSaver.save(tfActiveSession.tfoSession, self.sReplayBufferRestorePath)\n",
    "\n",
    "    # Restore model\n",
    "    def restore(self):\n",
    "        try:\n",
    "            self.oTargetSaver.restore(tfActiveSession.tfoSession, self.sTargetRestorePath)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.oTrainSaver.restore(tfActiveSession.tfoSession, self.sTrainRestorePath)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.oReplayBufferSaver.restore(tfActiveSession.tfoSession, self.sReplayBufferRestorePath)\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    # Perform one training cycle\n",
    "    def train(self):\n",
    "        fStartTime = time.time()\n",
    "        fTotalLoss = tfEval(self.tfnTrain)\n",
    "        fTimeElapsed = time.time() - fStartTime\n",
    "\n",
    "        return fTotalLoss, fTimeElapsed\n",
    "\n",
    "    # Flush all write buffers before closing\n",
    "    def flush(self):\n",
    "        tfEval(self.oSummaryWriter.flush())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T20:46:32.827452Z",
     "start_time": "2020-06-29T17:51:53.557429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MEM:468791296:3907833856] Fill: Ep.1:98, Score -201.759663, RewardAvg -0.196427, Used 98 of 131072        \n",
      "[MEM:468791296:3907833856] Fill: Ep.2:69, Score -335.363813, RewardAvg -0.332597, Used 167 of 131072        \n",
      "[MEM:468791296:3907833856] Fill: Ep.3:63, Score -101.148985, RewardAvg -0.100947, Used 230 of 131072        \n",
      "[MEM:468791296:3907833856] Fill: Ep.4:88, Score -409.718291, RewardAvg -0.399650, Used 318 of 131072        \n",
      "[MEM:470147072:3907833856] Fill: Ep.5:98, Score -149.299674, RewardAvg -0.147940, Used 416 of 131072        \n",
      "[MEM:470147072:3907833856] Fill: Ep.6:69, Score -99.570018, RewardAvg -0.099234, Used 485 of 131072        \n",
      "[MEM:470147072:3908096000] Fill: Ep.7:72, Score -85.722617, RewardAvg -0.084817, Used 557 of 131072        \n",
      "[MEM:470147072:3908096000] Fill: Ep.8:138, Score -402.452137, RewardAvg -0.388593, Used 695 of 131072        \n",
      "[MEM:470147072:3908096000] Fill: Ep.9:119, Score -468.703400, RewardAvg -0.455598, Used 814 of 131072        \n",
      "[MEM:470147072:3908096000] Fill: Ep.10:119, Score -206.816219, RewardAvg -0.201574, Used 933 of 131072        \n",
      "[MEM:470147072:3908096000] Fill: Ep.11:73, Score -111.878312, RewardAvg -0.111158, Used 1006 of 131072        \n",
      "[MEM:470147072:3908096000] Fill: Ep.12:123, Score -312.851682, RewardAvg -0.304731, Used 1129 of 131072        \n",
      "\n",
      "Status: Used 1129 of 131072 from 0 to 1129\n",
      "[MEM:483524608:3913551872] Fill: Ep.13:67, Score -269.727159, RewardAvg -0.264986, Used 1196 of 131072        \n",
      "[MEM:516644864:3920039936] Fill: Ep.14:76, Score -97.736032, RewardAvg -0.097108, Used 1272 of 131072        \n",
      "[MEM:524484608:3920039936] Fill: Ep.15:107, Score -408.622134, RewardAvg -0.395871, Used 1379 of 131072        \n",
      "\n",
      "Keyboard Interrupt\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "oEnvironment = CustomEnvironment(bRender=bRender)\n",
    "# Define active graph\n",
    "with tfGraph() as tfoGraph:\n",
    "    # Create neural network `Target Actor`\n",
    "    nnActor = CreateActor(oEnvironment, 1)\n",
    "    # Create virtual player\n",
    "    oRandomPlayer = CustomPlayer(oEnvironment, nnActor, fnSelectNoisyRandom)\n",
    "    # Create `Soft Actor Critic` agent\n",
    "    oAgent = SacAgent(oRandomPlayer,\n",
    "        uReplayCapacity=uReplayCapacity,\n",
    "        fRatingDiscountFactor=fRatingDiscountFactor,\n",
    "        uBatchSize=uBatchSize,\n",
    "        bEpisodeRating=bEpisodeRating,\n",
    "        bPriorityMode=bPriorityMode,\n",
    "        fTrainableTargetAverageUpdateCoef=fTrainableTargetAverageUpdateCoef,\n",
    "        uLogLevel=uLogLevel,\n",
    "        sLogsPath='logs/' + sName + '/',\n",
    "        sRestorePath='models/' + sName + '/')\n",
    "\n",
    "    # Start session calculating graph\n",
    "    with tfSession(tfoGraph):\n",
    "        # Initialize global variables\n",
    "        tfInitGlobal()\n",
    "        # Initialize local variables\n",
    "        tfInitLocal()\n",
    "        # Initialize agent\n",
    "        oAgent.initialize()\n",
    "        if not bRestore or not oAgent.restore():\n",
    "            # Pre-fill the replay buffer with initial data\n",
    "            oAgent.fill(uBatchSize * 4, uDebugLevel=2, bPrefill=True)\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                # Fill the repeat buffer in one step or one episode, depending on the setting\n",
    "                uFillCount, bDone = oAgent.fill(1, uDebugLevel=1)\n",
    "                # For each new step in the replay buffer, do a train\n",
    "                while uFillCount > 0:\n",
    "                    fTotalLoss = oAgent.train()\n",
    "                    uFillCount -= 1\n",
    "                # Save model\n",
    "                if bDone:\n",
    "                    oAgent.save()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nKeyboard Interrupt')\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Flush all write buffers before closing\n",
    "            oAgent.flush()\n",
    "            # Close environment\n",
    "            oEnvironment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
